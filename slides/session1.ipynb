{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Introduction to eye-tracking\"\n",
        "author: \"Badaya & Baltais\"\n",
        "format: \n",
        "   clean-revealjs:\n",
        "    logo: /_images/logo_mils.png\n",
        "    footer: \"Introduction to eye-tracking\"\n",
        "    include-in-header: \n",
        "      text: |\n",
        "        <style>\n",
        "        .center-xy {\n",
        "          margin: 0;\n",
        "          position: absolute;\n",
        "          top: 40%;\n",
        "          -ms-transform: translateY(-50%), translateX(-50%);\n",
        "          transform: translateY(-50%), translateX(-50%);\n",
        "        }\n",
        "        </style>\n",
        "editor: visual\n",
        "---"
      ],
      "id": "3450e79c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Welcome to the course\n",
        "\n",
        "## Welcome to the course\n",
        "\n",
        "Teaching team\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "-   Esperanza Badaya\n",
        "    -   Research focus: Speech comprehension \n",
        "    -   esperanza.badaya\\@ugent.be\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "-   Mariia Baltais\n",
        "    -   Research focus: Reading \n",
        "    -   mariia.baltais\\@ugent.be\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Welcome to the course\n",
        "\n",
        "What about yourselves?\n",
        "\n",
        "-   Name & uni/department\n",
        "-   Research interest\n",
        "-   Goal for this course?\n",
        "\n",
        "/!\\ Please fill in this survey.\n",
        "\n",
        "## Overview of the course\n",
        "\n",
        "Learning objectives\n",
        "\n",
        "-   Basic knowledge of how eye-trackers work and how to ensure high quality data\n",
        "    -   Practice with EyeLink 1000+\n",
        "-   Understanding of eye-tracking measurements and their meaning in psycholinguistics\n",
        "-   Introduction to the Visual World Paradigm and reading.\n",
        "    -   Set up, confounds, measures of interest.\n",
        "-   Introduction to eye-tracking data from raw data to analysis.\n",
        "    -   Data pre-processing, wrangling, models and visualization.\n",
        "    \n",
        "## Overview of the course\n",
        "\n",
        "Materials (all available on Ufora and GitHub)\n",
        "\n",
        "-   Slides\n",
        "-   Template scripts in PsychoPy\n",
        "-   Eye-tracking datasets\n",
        "-   Further resources (papers & book chapters)\n",
        "\n",
        "## Overview of the course\n",
        "\n",
        "Requirements\n",
        "\n",
        "- Install PsychoPy \n",
        "-   Install R, RStudio\n",
        "    -   version 4.4.1\n",
        "    -   packages tidyverse, lme4\n",
        "-   Install DataViewer\n",
        "    -   version 4.4.1\n",
        "-   Readings\n",
        "    -   Experimental design .pdf\n",
        "    -   Recommended readings per day\n",
        "    \n",
        "## Overview of the course\n",
        "\n",
        "| Date  |              Content              |                 Materials                  |\n",
        "|:-------------:|:-----------------------:|:------------------------------:|\n",
        "| Day 1 |   Introduction to eye-tracking    |                    None                    |\n",
        "| Day 2 |       Visual World Paradigm       | PsychoPy, DataViewer, 2 readings, datasets |\n",
        "| Day 3 |              Reading              | DataViewer, 2 readings, datasets           |\n",
        "| Day 4 | Lab session                       |            DataViewer, PsychoPy            |\n",
        "| Day 5 |  Practice with data & analysis    |                R                           |\n",
        "\n",
        "## Why do we move our eyes? The Human Visual System\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br> Important parts of the eye's anatomy:\n",
        "\n",
        "-   Cornea\n",
        "-   Pupil\n",
        "-   Retina\n",
        "    -   Fovea\n",
        "    -   Parafovea\n",
        "    -   Periphery\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/eye_anatomy.png)\n",
        ":::\n",
        ":::\n",
        "  \n",
        "## Why do we move our eyes? The Human Visual System\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br>\n",
        "\n",
        "-   Light hits the cornea.\n",
        "    -   Some light is reflected (Purkinje reflections)\n",
        "-   Light enters the eye via the pupil.\n",
        "-   The lens reflects the light onto the retina.\n",
        "    -   The visual axis hits the center of the fovea (1/2 degrees of visual angle).\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/eye_anatomy.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Why do we move our eyes? The Human Visual System\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br>\n",
        "\n",
        "The fovea is a small section of our [visual field]{.fg style=\"--col: #e64173\"}.\n",
        "\n",
        "-   Space respect to our eyes that we can perceive.\n",
        "-   Measured in visual angles.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/central_vision.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Why do we move our eyes? The Human Visual System\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "Retina:\n",
        "\n",
        "-   Fovea: 1-2 degrees of visual angle.\n",
        "-   Parafovea: 10 degrees of visual angle to either side.\n",
        "-   Peripheria: Remaining space beyond the parafovea.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "::: {layout=\"[[-1], [1], [-1]]\"}\n",
        "![Central vision != foveal vision.](_images/_session1/vision_angles.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Why do we move our eyes? The Human Visual System\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br>\n",
        "\n",
        "-   Light hits the cornea.\n",
        "    -   Some light is reflected (Purkinje reflections)\n",
        "-   Light enters the eye via the pupil.\n",
        "-   The lens reflects the light onto the retina.\n",
        "    -   Photosensitive layer with [cones]{.fg style=\"--col: #e64173\"} and [rodes]{.fg style=\"--col: #e64173\"}.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/eye_anatomy.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Why do we move our eyes? The Human Visual System\n",
        "\n",
        "Photoreceptors with different properties (e.g., spectral sensitivity, photopigments).\n",
        "\n",
        "-   Cones\n",
        "    -   [Color vision and spatial frequency]{.fg style=\"--col: #e64173\"} (\"visual details\").\n",
        "    -   Well-illuminated conditions (photopic vision).\n",
        "-   Rods\n",
        "    -   [Black and white]{.fg style=\"--col: #e64173\"}.\n",
        "    -   Low-light conditions (scotopic vision).\n",
        "\n",
        "## Why do we move our eyes? The Human Visual System\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "They also differ in where they are located in the fovea.\n",
        "\n",
        "-   Cones: Highest density in the [fovea]{.fg style=\"--col: #e64173\"}.\n",
        "-   Rodes: Higher density in the [fovea's periphery]{.fg style=\"--col: #e64173\"}.\n",
        "\n",
        "[Consequence]{.fg style=\"--col: #e64173\"}: Vision is sharpest in the fovea.\n",
        "\n",
        "-   25% of visual cortex devoted to processing 2.5° of the visual scene.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/rod_distribution.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Why do we move our eyes? The Human Visual System\n",
        "\n",
        "<br>\n",
        "\n",
        "::: {style=\"text-align: center\"}\n",
        "Therefore, we move our eyes to place visual stimuli in the fovea to process it with the highest acuity. [Eye movements are a consequence of the eyes' anatomy]{.fg style=\"--col: #e64173\"}.\n",
        ":::\n",
        "\n",
        "-   Parafoveal processing: No acute image, words still partially recognizable.\n",
        "-   Peripheria: Blurred image, no word/letter recognition.\n",
        "\n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "What do you notice about the eye movements here? What do you infer from them?\n",
        "\n",
        "[Playing a video game](https://www.youtube.com/watch?v=jzeBKRjWVwE)\n",
        "\n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "[Attention]{.fg style=\"--col: #e64173\"} (i.e., [linking hypothesis]{.fg style=\"--col: #e64173\"})\n",
        "\n",
        "-   Tracking eye movements can tell us what viewers are paying attention to.\n",
        "\n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "Attention determines what we process and the detail of the representation built.\n",
        "\n",
        "\n",
        "{{< video https://www.youtube.com/watch?v=U1saQoMRD8A width=\"70%\" height=\"70%\" >}}\n",
        "\n",
        "\n",
        "\n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "[Attention]{.fg style=\"--col: #e64173\"} (i.e., [linking hypothesis]{.fg style=\"--col: #e64173\"})\n",
        "\n",
        "-   [Bottom-up]{.fg style=\"--col: #e64173\"} and [top-down]{.fg style=\"--col: #e64173\"} processes.\n",
        "    -   Details that attract individuals' attention (exogeneous) v. Individuals' strategies (endogeneous).\n",
        "-   Individuals as active viewers.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![Open University](_images/_session1/endogenous.jpg)\n",
        ":::\n",
        ":::\n",
        "  \n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "- EXAMPLE HEATMAP -\n",
        "  \n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "Yarbus (1960): Scanpaths guided by attention.\n",
        "\n",
        "![They Did Not Expect Him, Iliá Repin](_images/_session1/unexpected_visitor.png){fig-align=\"center\"}\n",
        "\n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "::: {layout=\"[15,-2,10]\" layout-valign=\"bottom\"}\n",
        "![](_images/_session1/unexpected_visitor.png)\n",
        "\n",
        "![Free viewing](_images/_session1/yarbus_freeviewing.png)\n",
        ":::\n",
        "\n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "::: {layout=\"[15,-2,10]\" layout-valign=\"bottom\"}\n",
        "![](_images/_session1/unexpected_visitor.png)\n",
        "\n",
        "![Estimate individuals' ages](_images/_session1/yarbus_ages.png)\n",
        ":::\n",
        "\n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "::: {layout=\"[15,-2,10]\" layout-valign=\"bottom\"}\n",
        "![](_images/_session1/unexpected_visitor.png)\n",
        "\n",
        "![Estimate what they were doing when the visitor arrived](_images/_session1/yarbus_guessactivity.png)\n",
        ":::\n",
        "\n",
        "## Why do we move our eyes? Visual attention\n",
        "\n",
        "Attention is a bridge between our minds and eye movements.\n",
        "\n",
        "[Potential caveat]{.fg style=\"--col: #e64173\"}\n",
        "\n",
        "-   Covert versus overt attention:\n",
        "    -   Covert: Mental shift without physical evidence (e.g., looking at the slides while thinking about lunch).\n",
        "    -   Overt: Moving your eyes to check what time it is.\n",
        "\n",
        "We only have access to overt attention.\n",
        "\n",
        "-   But they are highly interlinked.\n",
        "\n",
        "## Why do we do eye-tracking?\n",
        "\n",
        "Description v [cognitive processes]{.fg style=\"--col: #e64173\"}\n",
        "\n",
        "- Linking hypotheses & assumptions\n",
        "  - Often times, lack of an explicit link.\n",
        "  - Underlying assumptions not explicit.\n",
        "  \n",
        ". . .\n",
        "  \n",
        "Example in language: The [eye-mind hypothesis]{.fg style=\"--col: #e64173\"} (Just & Carpenter, 1980).\n",
        "    -   [Where]{.fg style=\"--col: #e64173\"} we look indicates [what we are processing]{.fg style=\"--col: #e64173\"}, [for how long we look]{.fg style=\"--col: #e64173\"} indicates the [cognitive effort it takes to process it]{.fg style=\"--col: #e64173\"}.\n",
        "    - Contestable.\n",
        "    - Underlying assumptions: Serial processing.\n",
        "\n",
        "## Why do we do it?\n",
        "\n",
        "-   Visual acuity is highest in the fovea, but the fovea is a rather small section of the retina.\n",
        "-   Moving our eyes helps us to 'place' objects on the fovea.\n",
        "-   Why do we want to place something there? Arguably, because we are interested in it.\n",
        "-   We move our eyes to what captures our attention to process it.\n",
        "\n",
        "# Eye movements\n",
        "\n",
        "## Nature of eye-movements\n",
        "\n",
        "-   One dominant eye[^2].\n",
        "-   Binocular disparity:\n",
        "    -   Relatively small in healthy subjects.\n",
        "    -   Decreases over the time of a fixation.\n",
        "    -   No complete temporal synchrony in eye movements.\n",
        "\n",
        "[^2]: More on this when we cover properly the lab set up.\n",
        "\n",
        "## Eye movements\n",
        "\n",
        "::: {style=\"text-align: center\"}\n",
        "**What eye movements can you think of?**\n",
        "\n",
        "::: incremental\n",
        "-   Think of how we talk about things: we [fixate]{.fg style=\"--col: #e64173\"} on things, we [move]{.fg style=\"--col: #e64173\"} our eyes.\n",
        "-   We also [blink]{.fg style=\"--col: #e64173\"}.\n",
        "-   We can also measure [pupil size]{.fg style=\"--col: #e64173\"}.\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Eye movements\n",
        "\n",
        "**What eye movements can you think of?**\n",
        "\n",
        "::: incremental\n",
        "-   Fixations.\n",
        "-   Saccades.\n",
        "-   Blinks.\n",
        "-   Smooth pursuit.\n",
        "-   Pupil size changes.\n",
        ":::\n",
        "\n",
        "## Eye movements: Fixations\n",
        "\n",
        "When our eye 'stops' i.e., multiple gaze points close in time and/or space.\n",
        "\n",
        "-   Eye is *relatively* stable.\n",
        "\n",
        "-   Average duration: 200 - 300 ms.\n",
        "\n",
        "-   Minimal duration: 20 - 50 ms (not standard).\n",
        "\n",
        "-   **Sampling frequency**\n",
        "\n",
        "## Eye movements: Fixations\n",
        "\n",
        "When our eye 'stops'.\n",
        "\n",
        "-   Eye is *relatively* stable.\n",
        "    -   Tremor, drifts, microsaccades.\n",
        "\n",
        "![](_images/_session1/fixation_tremor.png){fig-align=\"center\"}\n",
        "\n",
        "## Eye movements: Fixations\n",
        "\n",
        "![Alexander & Martinez-Conde (2019)](_images/_session1/micro_saccades.PNG)\n",
        "\n",
        "## Eye movements: Fixations\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "-   Tremor: Smallest of the movements.\n",
        "-   Drifts: Slow movement away from the center of the fixation, happens between microsaccades.\n",
        "-   Microsaccades: Move back the eye to the center of the fixation.\n",
        "    -   Perceptual fading: Troxler fading.\n",
        "    -   Only microsaccades can restore it, while drift and microsaccades prevent fading.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "<br> <br>\n",
        "\n",
        "![](_images/_session1/troxler.jpg){fig-align=\"center\" height=\"300px\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Eye movements: Saccades\n",
        "\n",
        "![](_images/_session1/fixation_saccades.png){fig-align=\"center\"}\n",
        "\n",
        "## Eye movements: Saccades\n",
        "\n",
        "\"Jerky\" movement: Fast movement of the eye, usually from one fixation to another.\n",
        "\n",
        "-   Temporary blindness (i.e., [saccadic suppression]{.fg style=\"--col: #e64173\"}).\n",
        "-   Reactive saccades versus Voluntary saccades.\n",
        "    -   Sudden appearance of an object versus Exploration.\n",
        "\n",
        "## Eye movements: Saccades\n",
        "\n",
        "Parameters\n",
        "\n",
        "-   Amplitude: distance travelled.\n",
        "    -   Average: 15°.\n",
        "-   Temporal parameters (onset, offset, duration).\n",
        "    -   Average duration: 30 - 80 ms.\n",
        "-   Direction: Forward and backwards (i.e., regressions) saccades.\n",
        "-   Accuracy: Over- and under-shooting.\n",
        "    -   In simple lab conditions, fall slightly short of the target, thus followed by a small corrective saccade.\n",
        "    \n",
        "## Eye movements: Saccades\n",
        "\n",
        "Forward and backwards (i.e., regressions) saccades.\n",
        "\n",
        "-   Short and long regressions.\n",
        "\n",
        "![Conklin et al. (2018)](_images/_session1/example_reading.JPG){fig-align=\"center\"}\n",
        "\n",
        "## Eye movements: Blinks\n",
        "\n",
        "By necessity, people blink.\n",
        "\n",
        "-   Usually, surrounded by saccades.\n",
        "-   Pupil changes when eyelids open/close.\n",
        "-   Lab conditions.\n",
        "- Cognitive effort.\n",
        "\n",
        "## Eye movements: Smooth pursuit\n",
        "\n",
        "A \"moving fixation\" \\~ following a target.\n",
        "\n",
        "-   Slower than a saccade, but bounded by the velocity of the target being followed.\n",
        "-   Asymmetrical: Horizontal \\> vertical.\n",
        "\n",
        "## Eye movements: Pupil size\n",
        "\n",
        "Pupil dilates for reasons other than light, e.g., [cognitive effort]{.fg style=\"--col: #e64173\"}.\n",
        "\n",
        "-   Linking hypothesis: Pupil size reflects effort exerted.\n",
        "\n",
        "    -   Harder tasks = increase in pupil size.\n",
        "\n",
        "-   Increasing interest in language research, e.g., accented-speech comprehension.\n",
        "\n",
        "-   Changes can take up to 3 s.\n",
        "\n",
        "    -   Far longer than other eye events.\n",
        "\n",
        "## Why does our pupil change in size? Taxonomy\n",
        "\n",
        "::: rows\n",
        "::: {.row height=\"5%\"}\n",
        "-   Taxonomy by Strauch et al. (2022)\n",
        "-   Different networks\n",
        ":::\n",
        "\n",
        "::: {.row height=\"95%\"}\n",
        "::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "**Low-level**\n",
        "\n",
        "-   Pupillary light reflex\n",
        "-   Pupil Near response\n",
        "-   Dark reflex\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "**Intermediate-level**\n",
        "\n",
        "-   Alerting responses\n",
        "-   Orienting responses\n",
        ":::\n",
        "\n",
        "::: {.column width=\"30%\"}\n",
        "**High-level**\n",
        "\n",
        "-   Mental arousal\n",
        ":::\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: notes\n",
        "Although we want to draw conclusions about higher-level factors, we also need to keep these other factors in mind, especially when designing experiments e.g., play a beep before stimulus onset would lead to an orienting response\n",
        ":::\n",
        "\n",
        "## Why does our pupil change in size? Taxonomy\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "**Low-level**\n",
        "\n",
        "-   Pupillary light reflex\n",
        "-   Pupil Near response\n",
        "-   Dark reflex\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "Reading/hearing words that convey brightness/darkness\n",
        "\n",
        "![Mathôt et al. (2017)](_images/_session1/mathotetal2017_embodiedcognition.PNG)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Why does our pupil change in size? Higher-level processes\n",
        "\n",
        "-   Allocation of resources\n",
        "\n",
        "![Chiew & Braver (2013)](_images/_session1/chiew_braver2.PNG)\n",
        "\n",
        "## Why does our pupil change in size? Higher-level processes\n",
        "\n",
        "![Fink et al. (2023)](_images/_session1/fink_review2.PNG)\n",
        "\n",
        "## Eye movements: Conceptualisation\n",
        "\n",
        "Eye movements can be later operationalised as a function of the research question.\n",
        "\n",
        "-   Space\n",
        "-   Space x Time\n",
        "\n",
        "## Eye movements: Space\n",
        "\n",
        "We talk about [Areas of Interest (AOI)]{.fg style=\"--col: #e64173\"}[^3].\n",
        "\n",
        "[^3]: Or Regions of Interest (ROI), Interest Area (IA)\n",
        "\n",
        "-   Use to explore eye movements around them.\n",
        "- /!\\ Consequences for experimental design and coding.\n",
        "\n",
        "![](_images/_session1/example_IA.jpg){fig-align=\"center\"}\n",
        "\n",
        "## Eye movements: Space x Time\n",
        "\n",
        "\n",
        "We can talk about [dwell time]{.fg style=\"--col: #e64173\"}[^4].\n",
        "\n",
        "[^4]: Dwell time can be later subdivided in first fixation duration, total time, etc.\n",
        "\n",
        "-   Amount of time an individual spends looking at a specific area.\n",
        "\n",
        "## Eye movements: Space x Time\n",
        "\n",
        "\n",
        "Speech comprehension\n",
        "\n",
        "-   When a fixation is triggered towards an object upon hearing its name.\n",
        "\n",
        "Reading\n",
        "\n",
        "-   Timed measures: first fixation duration, total reading time.\n",
        "    -   How long did a person spent reading a certain word the first time they saw it?\n",
        "    \n",
        "## Eye movements: Conceptualisation\n",
        "\n",
        "Field-specific\n",
        "\n",
        "-   Lots of measures & linking hypotheses\n",
        "-   Lots of ways to convey information\n",
        "\n",
        "::: notes\n",
        "Obviously, when we conduct an ET exp for eye movements, we may use this measurements 'raw', but in some cases, they can be further divided, and how they link to a cognitive process may differ as a function of the field\n",
        ":::\n",
        "  \n",
        "## Eye movements: Conceptualisation\n",
        "\n",
        "How do you think you can link these eye movements to language research?\n",
        "\n",
        "- What are the links?\n",
        "- What are your predictions?\n",
        "\n",
        "## Eye movements: Conceptualisation\n",
        "\n",
        "::: rows\n",
        "::: {.row height=\"40%\"}\n",
        "Lots of ways to convey information: Example: Reading\n",
        "\n",
        "-   Fixations and saccades grouped into early/intermediate/late measures\n",
        "    -   Which map onto different hypothesized processes\n",
        "    \n",
        "- /!\\ Linking hypothesis\n",
        ":::\n",
        "\n",
        "::: {.row height=\"60%\"}\n",
        "![Carroll (2017)](_images/_session1/reiterate.JPG)\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: notes\n",
        "Not going into this, I will only mention this briefly e.g., in reading, for example, the duration of fixations can be subdivided into the first fixation, the amount of fixations on an area of interest etc. and those are believed to map onto different stages of reading comprehension\n",
        "Ask yourself: What are your predictions?\n",
        ":::\n",
        "  \n",
        "## Eye movements\n",
        "\n",
        "-   There are five major eye movements (or events) that an eye-tracker can capture.\n",
        "-   Fixations refer to 'stable' gazes on a space for a sustained period of time. We usually describe them in terms of how many they are, when they start, and how long they are.\n",
        "-   Saccades are fast movements, commonly from one fixation to another. We describe them in terms of onset, offset, angle, velocity, latency, and acceleration.\n",
        "-   Smooth pursuits are fixations that move.\n",
        "-   Pupil size can change due to cognitive processing, whereby its size increases when effort is exerted.\n",
        "\n",
        "## Is eye-tracking suitable for us?\n",
        "\n",
        "Eye-tracking is an *online* measurement of cognitive processes.\n",
        "\n",
        "-   Different online techniques e.g., EEG, fMRI\n",
        "    -   Spatial versus temporal resolution.\n",
        "-   Offline measurements e.g., comprehension questions.\n",
        "\n",
        "# How do eye-trackers work?\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "Old, rudimentary eye-trackers.\n",
        "\n",
        "-   Louis Émile Javal (1879)\n",
        "    -   'Naked eye' observations.\n",
        "    -   Stop-start pattern in reading.\n",
        "-   Edmund Huey (1898)\n",
        "    -   Primitive 'eye-tracking' device.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "<br> <br>\n",
        "\n",
        "![Huey, 1898; from Hutton, 2019](_images/_session1/huey.JPG)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br> <br> <br>\n",
        "\n",
        "-   Alfred Yarbus\n",
        "    -   Suction cups reflecting onto a photosensitive surface.\n",
        "    -   Scan paths.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "<br>\n",
        "\n",
        "![](_images/_session1/oldtrackers.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "Now you can see why nowadays eye-tracking is non-invasive.\n",
        "\n",
        "::: {layout=\"[15,-2,10]\"}\n",
        "![](_images/_session1/trackers_today.jpg)\n",
        "\n",
        "![](_images/_session1/headmounted_tracker.png)\n",
        ":::\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br> <br> ![](_images/_session1/trackers_today.jpg)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "::: rows\n",
        "::: {.row width=\"50%\"}\n",
        "![](_images/_session1/tracking_eyes2.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.rows width=\"50%\"}\n",
        "![](_images/_session1/et_work.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "  \n",
        "## How do eye-trackers work?\n",
        "\n",
        "-   Detects gaze and records its x and y coordinates on the screen.\n",
        "-   Parse gaze position into eye events, e.g., any gaze points nearby close in time are grouped into a *fixation*.\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "[Detects gaze]{.fg style=\"--col: #e64173\"} and records its x and y coordinates on the screen.\n",
        "\n",
        "-   Video-based recording of the location of either one point, the pupil, or two points, the pupil and the corneal reflection.\n",
        "-   P-CR ([the pupil and the corneal reflection]{.fg style=\"--col: #e64173\"}) is the most common.\n",
        "    -   Two hardware components: A camera and an infrared illuminator (which are fixed in space).\n",
        "    -   [1$^{st}$ corneal reflection (Purkinje reflection 1)]{.fg style=\"--col: #e64173\"}.\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "How do (most of them) work?\n",
        "\n",
        "-   Video-based recording of the location of two points: [the pupil and the corneal reflection]{.fg style=\"--col: #e64173\"}.\n",
        "    -   Infrared light is reflected on the participant's eyes.\n",
        "    -   Camera picks up the corneal reflection.\n",
        "    -   Algorithm-based image processing to identify these two locations\n",
        "\n",
        "![](_images/_session1/tracking_eyes3.png){fig-align=\"center\"}\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "Why pupil and corneal reflection?\n",
        "\n",
        ". . .\n",
        "\n",
        "-   A 'two-points of reference' system.\n",
        "    -   The pupil moves when we move our eyes (and so does its location on the camera).\n",
        "    -   But corneal reflection does not (because the light source does not move).\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "<br>\n",
        "\n",
        "![](_images/_session1/tracking_eyes1.png){fig-align=\"center\" width=\"400\"}\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "![](_images/_session1/cr_example2.PNG){fig-align=\"center\" width=\"400\"}\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "::: rows\n",
        "::: {.row height=\"50%\"}\n",
        "![Participant moving their eyes](_images/_session1/tracking_eyes4.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.row height=\"50%\"}\n",
        "![Participant moving their head](_images/_session1/cr_example3.PNG){fig-align=\"center\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "Why pupil and corneal reflection?\n",
        "\n",
        "-   A 'two-points of reference' system.\n",
        "    -   Distance between pupil and corneal reflection changes with eye rotation, but is relatively [constant]{.fg style=\"--col: #e64173\"} with head movements.\n",
        "    -   Pupil position - CR position.\n",
        "    \n",
        "## How do eye-trackers work?\n",
        "\n",
        "Monocular or binocular recording?\n",
        "\n",
        "- Usually: One eye (assumption: synchrony).\n",
        "- **But**: Depends on your research question (e.g., microsaccades), compensate for data loss.\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "Detects gaze and [records]{.fg style=\"--col: #e64173\"} its x and y coordinates on the screen.\n",
        "\n",
        "-   How many times? [Sampling frequency]{.fg style=\"--col: #e64173\"}\n",
        "\n",
        "## How do eye-trackers work?: Sampling frequency\n",
        "\n",
        "[Sampling frequency]{.fg style=\"--col: #e64173\"} (or rate): Number of times the tracker measures gaze position per second. Measured in hertz (Hz).\n",
        "\n",
        "-   300 Hz: 1 data point every \\~3 ms, 300 samples per second.\n",
        "    -   1 s is 1000 ms.\n",
        "    -   1000/300 = 3.33\n",
        "-   Higher sampling frequency = more data points, and closer in time.\n",
        "\n",
        "## How do eye-trackers work?: Sampling frequency\n",
        "\n",
        "**Question**\n",
        "\n",
        "::: incremental\n",
        "-   If our sampling frequency is 500 Hz, what is the time elapsed between each data point?\n",
        "    -   2 ms.\n",
        "-   And at 60 Hz?\n",
        "    -   \\~16 ms.\n",
        "-   And 2000 Hz?\n",
        "    -   0.5 ms.\n",
        ":::\n",
        "\n",
        "## How do eye-trackers work?: Sampling frequency\n",
        "\n",
        "Relationship between sampling frequency and measure of interest.\n",
        "\n",
        "-   The [faster]{.fg style=\"--col: #e64173\"} it is, the [higher]{.fg style=\"--col: #e64173\"} the sampling frequency needed to detect it.\n",
        "    -   Onset, offset, duration.\n",
        "\n",
        "Example: 50 Hz sampling frequency.\n",
        "\n",
        "-   50 samples in a second, one every 20 ms.\n",
        "    -   A saccade starts in these 20 ms, we miss it.\n",
        "    -   A fixation begins in these 20 ms, we get it, but misrepresent its onset.\n",
        "\n",
        "## How do eye-trackers work?: Sampling frequency\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "![Measurement error represented by dashed line; picture taken when the vertical lines cross the horizontal lines.](_images/_session1/sample_rate.jpg){fig-align=\"center\"}\n",
        "\n",
        "## How do eye-trackers work?: Sampling frequency\n",
        "\n",
        "[This is why sampling frequency matters]{.fg style=\"--col: #e64173\"}.\n",
        "\n",
        "-   Depends on your measure of interest, e.g., reading versus observing a scene.\n",
        "  -   [Reading]{.fg style=\"--col: #e64173\"} usually requires [higher]{.fg style=\"--col: #e64173\"} sampling frequency because of the measures of interest.\n",
        "  \n",
        "## How do eye-trackers work?: Sampling frequency\n",
        "\n",
        "How small (or slow) can you go with the sampling frequency?\n",
        "\n",
        "-   Nyquist-Shannon sampling theorem\n",
        "    -   Twice as large as the event you want to measure.\n",
        "    \n",
        "In reality, more like 'rule-of-thumb'.\n",
        "\n",
        "## How do eye-trackers work?: Sampling frequency\n",
        "\n",
        "How small (or slow) can you go with the sampling frequency?\n",
        "\n",
        "Relationship between sampling error, sampling frequency, and [data points]{.fg style=\"--col: #e64173\"}.\n",
        "\n",
        "-   Lower sampling frequencies => more data points (power).\n",
        "    -   NB: More data points might not be the solution.\n",
        "\n",
        "Conventional: min. 500 Hz for picture viewing, min. 1000 Hz for reading.\n",
        "\n",
        "::: notes\n",
        "for onset - offset, more data points is not a solution, for duration it may increase power\n",
        ":::\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "Detects gaze and records its [x and y coordinates]{.fg style=\"--col: #e64173\"} on the screen.\n",
        "\n",
        "-   How do we know that the x and y detected are good? Accuracy and precision\n",
        "-   [Accuracy]{.fg style=\"--col: #e64173\"}: Difference between the measurement and its true value.\n",
        "-   [Precision]{.fg style=\"--col: #e64173\"}: Ability to reproduce a reliable measurement.\n",
        "\n",
        "![From Holmqvist et al., 2011](_images/_session1/precision_accuracy.JPG){fig-align=\"center\" height=\"300px\"}\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "![Nyström et al., 2025](_images/_session1/example_dataquality.jpg)\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "Detects gaze and records its [x and y coordinates]{.fg style=\"--col: #e64173\"} on the screen.\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "Accuracy\n",
        "\n",
        "-   Recorded gaze position versus the true gaze position.\n",
        "-   Methods to increase accuracy in the experimental session.\n",
        "    -   Calibration, validation, drift correction.\n",
        "    -   Screen corners.\n",
        "-   Data loss.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "Precision\n",
        "\n",
        "-   Spatial precision: Eye fixating on a stationary target.\n",
        "-   Temporal precision: Standard deviation of delays from actual movements until they are marked (eye-tracker latencies).\n",
        ":::\n",
        ":::\n",
        "  \n",
        "## How do eye-trackers work? Accuracy\n",
        "\n",
        "[Accuracy]{.fg style=\"--col: #e64173\"}: Difference between the measurement and its true value.\n",
        "\n",
        "-   Measured in visual angles\n",
        "  - 0.5 visual anles.\n",
        "![](_images/_session1/accuracy.JPG)\n",
        "\n",
        "## How do eye-trackers work? Accuracy\n",
        "\n",
        "Good enough accuracy?\n",
        "\n",
        "- Reseach question e.g., AOI\n",
        "  - Size\n",
        "  - Location\n",
        "  \n",
        "## How do eye-trackers work? Accuracy\n",
        "\n",
        "-- ADD EXAMPLES SIZE AND LOCATION --\n",
        "\n",
        "## How do eye-trackers work? Precision\n",
        "\n",
        "[Precision]{.fg style=\"--col: #e64173\"}: Ability to reproduce a reliable measurement. <br>\n",
        "- Noise in the data\n",
        "\n",
        "![From Holmqvist et al., 2011](_images/_session1/precision_accuracy.JPG){fig-align=\"center\" height=\"300px\"}\n",
        "\n",
        "## How do eye-trackers work? Precision\n",
        "\n",
        "Spatial *and* temporal precision.\n",
        "\n",
        "[Spatial precision]{.fg style=\"--col: #e64173\"}: Eye fixating on a stationary target.\n",
        "\n",
        "![](_images/_session1/precision.JPG)\n",
        "\n",
        "## How do eye-trackers work? Latency {.smaller}\n",
        "\n",
        "[Eye-tracker latency]{.fg style=\"--col: #e64173\"} (related to temporal precision): Delay from actual movement until it is marked.\n",
        "\n",
        "-   End to end delay.\n",
        "\n",
        "![](_images/_session1/et_latency.JPG){fig-align=\"center\"}\n",
        "\n",
        "-   Low particularly important for gaze-contingent studies.\n",
        "\n",
        "Stimulus-synchronization latency: Stimulus presentation and recording software.\n",
        "\n",
        "-   Can affect analysis\n",
        "\n",
        "::: notes\n",
        "However, clocks on the two computers may run differently, and signals may be delayed at ports for a variety of reasons\n",
        ":::\n",
        "\n",
        "## How do eye-trackers work? Precision\n",
        "\n",
        "[Temporal precision]{.fg style=\"--col: #e64173\"}: Standard deviation of eye-tracker latencies.\n",
        "\n",
        "-   High temporal precision: Interval between samples is constant.\n",
        "-   High temporal precision required for gaze-contingent studies.\n",
        "\n",
        "::: notes\n",
        "A high temporal precision means that even if the samples arrive with a latency, the interval between successive samples remains almost constant. If the temporal precision is low, you have a variable latency and a variable delay in the synchronization to external software such as stimulus programs or auxiliary recordings such as EEG The cause behind variable latencies is usually that the recording computer allows processes---such as hard disk operations---to take up processor time and even get priority over the gaze data calculations. Again, for gaze-contingent tasks such as boundary crossing or simulated scotoma experiments, it is enough that the processor is occupied for a short instance, and the participant will detect an anomaly in the experiment.\n",
        ":::\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "These properties impact data quality.\n",
        "\n",
        "Why do we care about data quality?\n",
        "\n",
        "-   The [validity of results]{.fg style=\"--col: #e64173\"} based on eye movement analysis are clearly dependent on the quality of eye movement data (Holmqvist et al., 2012)\n",
        "\n",
        "Understanding how the tracker works can help solve problems while gathering data.\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "Different eye-tracking systems in the market.\n",
        "\n",
        "-   SR Research, Tobii, SMI.\n",
        "\n",
        "Differences in software e.g., parsing of events and subsequent measures.\n",
        "\n",
        "-   Research question\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "Different types of trackers in the market as a function of:\n",
        "\n",
        "-   Position camera wrt eye.\n",
        "\n",
        "-   Lens zoom.\n",
        "\n",
        "-   Software for parsing events.\n",
        "\n",
        "-   Pros and cons of each.\n",
        "\n",
        "    -   e.g., they differ in sampling frequency (head mounted tend to have lower sampling frequencies).\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "<br>\n",
        "\n",
        "![](_images/_session1/tracker_types.png){fig-align=\"center\"}\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "Table-mounted, head fixed:\n",
        "\n",
        "-   Assumed constant distance.\n",
        "    -   Camera and infrared light are above the participant's head or fixed position near the monitor (desktop mounted).\n",
        "-   Head movement is restricted.\n",
        "-   Can be tower-mounted, monitor-mounted or desktop-mounted with head stabilized.\n",
        "-   Pros: High precision.\n",
        "-   Cons: Not suitable for all populations.\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "Table-mounted, head free:\n",
        "\n",
        "-   Distance changes.\n",
        "\n",
        "-   Range of head movement.\n",
        "\n",
        "    -   Head distance.\n",
        "\n",
        "-   Can be desktop-mounted or monitor-mounted\n",
        "\n",
        "-   Pros: Generally, high precision; suitable for more populations.\n",
        "\n",
        "-   Cons: The lack of head restriction can lower data precision.\n",
        "\n",
        "-   Portable trackers.\n",
        "\n",
        "-   Convertible trackers.\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "Head-mounted:\n",
        "\n",
        "-   Camera and infrared light are mounted on the participant's head (e.g., with glasses)\n",
        "-   Pros: Ecological validaity.\n",
        "-   Cons: Usually, lower sampling frequencies, lower accuracy.\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "Eye-tracking without an eye-tracker.\n",
        "\n",
        "![](_images/_session1/alternative_trackers.png){fig-align=\"center\"}\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "Eye-tracking without an eye-tracker.\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "![King et al., 2019](_images/_session1/mt_1.png){fig-align=\"center\" height=\"350px\"}\n",
        "\n",
        "![](_images/_session1/mt_2.png){fig-align=\"center\" height=\"350px\"}\n",
        ":::\n",
        "\n",
        "If interested, see Spivey (2025)\n",
        "\n",
        "## How do eye-trackers work? Types\n",
        "\n",
        "Mouse-tracking (online), MoTR (Wilcox et al., 2024)\n",
        "\n",
        "-   Imitate the perceptual span with a cursor\n",
        "-   Some differences with eye-tracking...\n",
        "\n",
        "[Have a go yourselves!](https://wilcoxeg.github.io/MoTR/MoTR/run_motr_in_magpie/demo/)\n",
        "\n",
        "## How do eye-trackers work? Converging measures\n",
        "\n",
        "Combination with other techniques.\n",
        "\n",
        "![](_images/_session1/combining_tracking.png){fig-align=\"center\" height=\"300px\"}\n",
        "\n",
        "## How do eye-trackers work?\n",
        "\n",
        "-   We track the x and y coordinates of gaze on a screen by figuring out the position of either the pupil or the pupil and the corneal reflection.\n",
        "-   How many times we record the x and y coordinates is determined by the sampling frequency.\n",
        "-   Our sampling frequency is determined by our measure of interest.\n",
        "-   We distinguish eye-trackers depending on the distance between the eyes and the camera.\n",
        "\n",
        "## Eye-tracking: Pros\n",
        "\n",
        ". . .\n",
        "\n",
        "-   Widely applicable technique.\n",
        "-   Relatively easy to interpret.\n",
        "-   Can provide a huge range of online measures.\n",
        "-   Temporal precision.\n",
        "-   Relatively high ecological validity.\n",
        "\n",
        "## Eye-tracking: Cons\n",
        "\n",
        ". . .\n",
        "\n",
        "-   Relatively expensive.\n",
        "    -   In terms of money: up to 30,000 e (but in comparison to EEG?).\n",
        "    -   In terms of time: one participant at a time[^5].\n",
        "-   Trade-off between accuracy and ecological validity.\n",
        "-   Participants' criteria.\n",
        "\n",
        "[^5]: Dual eye-tracking experiments are being done!\n",
        "\n",
        "# How is an eye-tracking experiment?\n",
        "\n",
        "## Basics of an experiment\n",
        "\n",
        "Goal: Ensure high quality data + Understand the basics of an eye-tracking experiment\n",
        "\n",
        "Two components:\n",
        "  - The eye-tracking lab.\n",
        "  - The experiment.\n",
        "\n",
        "## The eye-tracking lab\n",
        "\n",
        "-   Most eye-trackers nowadays are video-based recordings of pupil and corneal reflection.\n",
        "-   Two hardware components: A camera and an infrared illuminator.\n",
        "-   Gaze position is capture every X ms (depends on sample rate).\n",
        "\n",
        "Question is: How does an eye-tracking lab look like? What elements do you think the lab has?\n",
        "\n",
        "A camera, a screen, etc.\n",
        "\n",
        "## The eye-tracking lab\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br>\n",
        "\n",
        "Hardware\n",
        "\n",
        "-   2 PCs (Host and Presentation)\n",
        "-   2 Screens (Experimenter and Participants)\n",
        "-   Camera and infrared\n",
        "-   Peripheral hardware\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/lab_setup2.jpg){fig-align=\"center\"}\n",
        ":::\n",
        ":::\n",
        "  \n",
        "## The eye-tracking lab\n",
        "\n",
        "Hardware set up.\n",
        "\n",
        "-   Distance between participant, camera, and participant's screen are measured.\n",
        "    -   Distance participant - monitor\n",
        "    -   Distance participant - camera\n",
        "    -   Interplay with screen size.\n",
        "    -   [/!\\\\]{.bg style=\"--col: #e64173\"} Trackable range of the tracker\n",
        "    -   Constant across participants\n",
        "\n",
        "All in a sound isolated, no-sunlight room.  \n",
        "\n",
        "## The eye-tracking lab\n",
        "\n",
        "Host PC and camera are connected and synchronized. Host PC and Presentation PC are connected and synchronized.\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "-   Recording/Host PC\n",
        "    -   Camera set up.\n",
        "    -   Records triggers.\n",
        "    -   Processes and records eye-tracking data.\n",
        "    -   Observe eye movements in real time.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "-   Presentation PC\n",
        "    -   Presentation of stimuli.\n",
        "    -   Sends triggers and trial information.\n",
        "    -   Control when in time gaze is actually recorded.\n",
        "    -   Gathers behavioural data.\n",
        ":::\n",
        ":::\n",
        "\n",
        "## The eye-tracking lab\n",
        "\n",
        "What information do you think is sent between the two computers?\n",
        "\n",
        "-   Areas of Interest\n",
        "-   Triggers\n",
        "-   Experimental condition\n",
        "-   Stimuli per trial\n",
        "-   ...\n",
        "\n",
        "[/!\\\\]{.bg style=\"--col: #e64173\"} Keep this in mind when coding the experiment.\n",
        "\n",
        "## The experiment\n",
        "\n",
        "How can I code an eye-tracking experiment?\n",
        "\n",
        ":::columns\n",
        ":::{.column width = \"30%\"}\n",
        "\n",
        "General commercial software:\n",
        "\n",
        "-   E-Prime\n",
        "-   Presentation\n",
        "-   PsyScope\n",
        "\n",
        "Advantages: User-friendly.\n",
        "\n",
        "Disadvantages: License.\n",
        "\n",
        ":::\n",
        ":::{.column width = \"30%\"}\n",
        "\n",
        "Commercial software associated with eye-tracking hardware\n",
        "\n",
        "-   SMI: Experiment Center\n",
        "-   Tobii: Tobii Studio\n",
        "-   EyeLink: Experiment Builder\n",
        "\n",
        "Advantages: User-friendly, created with eye-tracking experiments in mind.\n",
        "\n",
        "Disadvantages: License (but if you own an eye-tracker, you commonly have one).\n",
        "\n",
        ":::\n",
        ":::{.column width = \"30%\"}\n",
        "\n",
        "Programming environments\n",
        "\n",
        "-   MATLAB (Psychophysics Toolbox)\n",
        "-   Python (e.g., PsychoPy, OpenSesame)\n",
        "\n",
        "Advantages: Free software, nicely documented, versatile, user-friendly interfaces.\n",
        "\n",
        "Disadvantages: More complex functionalities require coding (\"more intimidating\").\n",
        "\n",
        ":::\n",
        ":::\n",
        "  \n",
        "## The experiment\n",
        "\n",
        "Focus of this course: **PsychoPy**\n",
        "\n",
        "- Templates on Ufora & GitHub\n",
        "- Minimal knowledge required\n",
        "  - Edit excel file (design & stimuli)\n",
        "  - Minimal changes with functions\n",
        "\n",
        "## The experiment\n",
        "\n",
        "Structure of an eye-tracking experiment\n",
        "\n",
        "1.  Set up\n",
        "\n",
        "2.  Experiment\n",
        "\n",
        "    -   Calibration & validation\n",
        "    -   Practice trials & familiarisation (optional)\n",
        "    -   Experimental phase (recording sequence, block division - optional)\n",
        "        -   (Drift correction) Stimulus --\\> Trigger --\\> Follow-up\n",
        "        -   Goodbye screen\n",
        "\n",
        "3.  After it\n",
        "\n",
        "## The experiment\n",
        "\n",
        "Beyond the experiment itself, all eye-tracking experiments share these steps.\n",
        "\n",
        "-   Calibration\n",
        "-   Validation\n",
        "-   Drift correction\n",
        "\n",
        "Why?\n",
        "\n",
        "-   Because of how gaze position is calculated.\n",
        "    -   Estimation from known coordinates = calibration points.\n",
        "-   Accuracy.\n",
        "\n",
        "## Calibration\n",
        "\n",
        "x,y coordinates of gaze position are estimated from measuring the pupil and the CR on known coordinates (i.e., the calibration points).\n",
        "\n",
        "::: r-stack\n",
        "\n",
        "{{< video _images/_session1/calibration_cut.mp4 height=\"500\" >}}\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "## Calibration\n",
        "\n",
        "-   Beginning of the experiment (blocks?)\n",
        "-   Fixate on a series of points - 9HV standard\n",
        "-   Interplay with kind of eye-tracker:\n",
        "    -   Portable versus fixed.\n",
        "\n",
        "::: notes\n",
        "more points for small areas of interest,\n",
        ":::\n",
        "\n",
        "## Calibration\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![Bad calibration](_images/_session1/bad_calibration.jpg){height=\"500px\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![Good calibration](_images/_session1/good_calibration.jpg){height=\"500px\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Calibration\n",
        "\n",
        "-- ADD IMAGE PSYCHOPY --\n",
        "\n",
        "## Validation\n",
        "\n",
        "Done after calibration to ensure accuracy. Same principle as calibration.\n",
        "\n",
        "::: r-stack\n",
        "\n",
        "{{< video _images/_session1/validation_cut.mp4 height=\"500\" >}}\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "## Validation\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![Bad validation](_images/_session1/bad_validation.jpg){height=\"500px\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![Good calibration](_images/_session1/good_validation.jpg){height=\"500px\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Validation\n",
        "\n",
        "-- ADD IMAGE PSYCHOPY --\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Technically speaking, you could now start recording.\n",
        "\n",
        "You'd get eye data of your experiment, but...\n",
        "\n",
        "-   How do we make sure that our calibration points are accurate throughout the experiment?\n",
        "-   How can we make sense of the data after the experiment?\n",
        "    -   Areas of Interest\n",
        "    -   Triggers\n",
        "\n",
        "## Drift correction\n",
        "\n",
        "Every so often, re-check our measurement (accuracy): Drift correction/check.\n",
        "\n",
        "-   Commonly, each trial/every other trial/blocks\n",
        "-   Ensure accuracy\n",
        "-   But cf. what it actually does - move grid?\n",
        "-   Where the fixation cross appears depends on your task.\n",
        "    -   At the position where you want high accuracy.\n",
        "    -   Reading: e.g., beginning of the sentence/paragraph.\n",
        "-   Trigger a calibration?\n",
        "\n",
        "## Drift correction\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![Drift correction reading](_images/_session1/drift_reading.jpg){height=\"500px\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![Drift correction VWP](_images/_session1/drift_vwp.jpg){height=\"500px\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Drift correction\n"
      ],
      "id": "0af9e473"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "while not dummy_mode:\n",
        "        if (not et_tracker.isConnected()) or et_tracker.breakPressed():\n",
        "            abort_exp()\n",
        "        try:\n",
        "            error = et_tracker.doDriftCorrect(int(1920/2.0),\n",
        "                                              int(1080/2.0), 1, 1)\n",
        "            # break following a success drift-check\n",
        "            if error is not pylink.ESC_KEY:\n",
        "                break\n",
        "        except:\n",
        "            pass"
      ],
      "id": "696df74f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Areas of Interest\n",
        "\n",
        "Areas of display we are interested in measuring and analysing.\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![Single sentence reading](_images/_session1/ia_reading.png){height=\"200px\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![VWP](_images/_session1/ia_vwp.png){height=\"300px\" fig-align=\"center\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Areas of Interest\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "Reading\n",
        "\n",
        "-   Word\n",
        "-   Sequence of words\n",
        "-   Part of a sentence\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "VWP\n",
        "\n",
        "-   Individual images\n",
        "-   Blank space!\n",
        ":::\n",
        ":::\n",
        "  \n",
        "## Areas of Interest\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "Reading\n",
        "\n",
        "Careful definition of AIs.\n",
        "\n",
        "-   Manually versus automatic (individual words)\n",
        "-   Issues: Segmentation of pre-critical and spillover regions.\n",
        "-   Issue: Duplicate data.\n",
        "-   Issue: Classification of eye-movements (regressions)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "VWP\n",
        "\n",
        "- Margins\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Areas of Interest\n",
        "\n",
        "Size depends on accuracy and precision of the eye-tracker.\n",
        "\n",
        "-   Low accuracy/precision: Bigger areas of interest.\n",
        "\n",
        "-   Do not put stimuli close to margins.\n",
        "\n",
        "    -   Fixations are generally drawn to the center of a screen.\n",
        "    -   Lower precision at margins of a monitor.\n",
        "\n",
        "-   Overlapping areas of interest.\n",
        "\n",
        "Particularly relevant for gaze-contingent paradigms!\n",
        "\n",
        "## Areas of Interest\n",
        "\n",
        "Important to consider for calibration and validation:\n",
        "\n",
        "-   Smaller AIs need more calibration points=\n",
        "\n",
        "## Areas of Interest\n",
        "\n",
        "-- ADD PICTURE PSYCHOPY --\n",
        "\n",
        "## Triggers\n",
        "\n",
        "Trigger: Information about an event, i.e., when it happens.\n",
        "\n",
        "Most basic trigger:\n",
        "\n",
        "-   Trial information (e.g., condition, response)\n",
        "\n",
        "But when you also have audio:\n",
        "\n",
        "-   When the audio begins and ends, gaze, for example.\n",
        "\n",
        "Related to stimulus-synchrony!\n",
        "\n",
        "## Triggers\n",
        "\n",
        "-- ADD PICTURE PSYCHOPY --\n",
        "\n",
        "# Eye-tracking in research\n",
        "\n",
        "## Eye-tracking in research\n",
        "\n",
        "Research traditions: Visual search (serial versus parallel processing), reading research (processes involved in text comprehension), scene perception (forming a representation of a scene), usability.\n",
        "\n",
        "-   Flash-preview moving-window paradigm, visual world paradigm, anti-saccadic paradigm, saccadic mislocalization, social interaction paradigm, change blingness paradigm, gaze-contigent paradigms, preferential-looking paradigms, prosaccade paradigm...\n",
        "\n",
        "    -   And combine other paradigms with eye-tracking\n",
        "\n",
        "::: notes\n",
        "I know there is a lot on this slide, and that's precisely its point! There are *many* things that can be done with ET. Some of these paradigms are ET specific, in that they are designed to either explore these measures we just discussed OR they actually depend on them! But also you can allegedly run many paradigms that are behavioural with ET, you may just need to make tweaks.\n",
        "\n",
        "We are now going to discuss a couple of these paradigms\n",
        ":::\n",
        "\n",
        "## Eye-tracking in language research\n",
        "\n",
        "Focus of this course:\n",
        "\n",
        "-   Visual World Paradigm (i.e., speech comprehension)\n",
        "-   Reading Paradigms (i.e., written text comprehension)\n",
        "\n",
        "## Eye-tracking in language research\n",
        "\n",
        "::: rows\n",
        "::: {.row height=\"40%\"}\n",
        "-   Visual World Paradigm (Cooper, 1974; Tanenhaus et al., 1995)\n",
        "\n",
        "    -   Concurrent presentation of auditory and visual stimuli\n",
        "    -   IV: Increase in fixations (but also: saccade latency, for example)\n",
        ":::\n",
        "\n",
        "::: {.row height=\"60%\"}\n",
        "::: columns\n",
        "::: {.column width=\"20%\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "{{< video _images/_session1/disfluent-native.mp4 width=\"500%\" height=\"500%\">}}\n",
        ":::\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: notes\n",
        "to explore how the latter affects the former cf. preview window\n",
        ":::\n",
        "\n",
        "## Eye-tracking in language research\n",
        "\n",
        "-   VWP has predominantly been used to investigate speech comprehension as it unfolds, to answer questions such as lexical access, prediction of upcoming elements in the speech signal, or even the interpretation of an utterance away from its propositional content.\n",
        "\n",
        "## Eye-tracking in language research\n",
        "\n",
        "Read (silently) a piece of text\n",
        "\n",
        "-   Can be a sentence, paragraphs etc.\n",
        "-   Huge datasets of eye movements data (e.g., GECO, Cop et al., 2017; MECO, Siegelman et al., 2022)\n",
        "\n",
        "\n",
        "{{< video https://www.youtube.com/watch?v=TwNNij89qro width=\"60%\" height=\"60%\" >}}\n",
        "\n",
        "\n",
        "\n",
        "## Eye-tracking in language research\n",
        "\n",
        "Read (silently) a piece of text\n",
        "\n",
        "-   We investigate naturalistic reading behaviour to make inferences about underlying linguistic processing mechanisms (lexical access, semantic and syntactic integration, prediction...)\n",
        "-   Stimuli of different length (single sentence vs. text), different tasks (skim for gist, answer specific questions, etc.)\n",
        "\n",
        "## Plan for tomorrow\n",
        "\n",
        "- Install DataViewer\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "## Extra slides\n",
        "\n",
        "## Structure of an eye-tracking experiment\n",
        "\n",
        "1.  Set up\n",
        "\n",
        "    -   Welcome participant, instructions\n",
        "        -   Including introduction to the eye-tracker\n",
        "    -   Check make up\n",
        "    -   Camera set up\n",
        "        -   Checks prior to experiment\n",
        "            -   Comfortable seating\n",
        "            -   Focus\n",
        "            -   Corners & areas of interest\n",
        "            -   Automatic & adjust thresholding\n",
        "\n",
        "2.  Experiment\n",
        "\n",
        "3.  After it\n",
        "\n",
        "## Structure of an eye-tracking experiment\n",
        "\n",
        "1.  Set up\n",
        "\n",
        "2.  Experiment\n",
        "\n",
        "    -   Calibration & validation\n",
        "    -   Practice trials & familiarisation (optional)\n",
        "    -   Experimental phase (recording sequence, block division - optional)\n",
        "        -   (Drift correction) Stimulus --\\> Trigger --\\> Follow-up\n",
        "        -   Goodbye screen\n",
        "\n",
        "3.  After it\n",
        "\n",
        "## Structure of an eye-tracking experiment\n",
        "\n",
        "1.  Set up\n",
        "\n",
        "2.  Experiment\n",
        "\n",
        "3.  After it\n",
        "\n",
        "    -   Post-experimental questionnaires & debriefing.\n",
        "\n",
        "## Participant set up\n",
        "\n",
        "Briefing\n",
        "\n",
        "-   Explain the experiment and allow them to ask questions.\n",
        "    -   Add instructions to the experiment & practice sessions.\n",
        "-   Let them know in advance the experiment structure (e.g., calibration \\> validation \\> experiment (practice if there is)).\n",
        "-   Explain (briefly) how the eye-tracker works.\n",
        "\n",
        ". . .\n",
        "\n",
        "Tempted to ask to not blink?\n",
        "\n",
        "-   Not blinking leads to dry eyes, and ultimately more blinking.\n",
        "-   Instead: Tell them when it is preferred for them to blink (if you decide to mention anything).\n",
        "\n",
        "## Participant set up\n",
        "\n",
        "Adjust seating\n",
        "\n",
        "-   Chinrest\n",
        "-   Chair height\n",
        "-   Sitting straight\n",
        "-   Head against foreheadrest, chin over chinrest.\n",
        "\n",
        "## Camera set up\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "Access to eye image\n",
        "\n",
        "-   Hidden image: set-up is automatic.\n",
        "    -   Easier to handle but potential for poorer data quality, e.g., data recording issues may go unnoticed.\n",
        "-   [Access]{.fg style=\"--col: #e64173\"} to image.\n",
        "    -   Need for technical knowledge.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/host_pc_setup.JPG)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Camera set up\n",
        "\n",
        "Which eye to track? - Dominant eye (by default, right eye)\n",
        "\n",
        "Detect the eye\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/eye_undetected.JPG){height=\"300px\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/eye_detected.JPG){height=\"300px\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Camera set up\n",
        "\n",
        "Focus\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/unfocused_camera.JPG){height=\"300px\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/eye_detected.JPG){height=\"300px\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Camera set up\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br> <br> <br>\n",
        "\n",
        "-   P-CR tracking:\n",
        "    -   Dark blue pupil\n",
        "    -   Light blue CR\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/good_captureeye1.png) ![](_images/_session1/good_pupil.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Camera set up\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "<br>\n",
        "\n",
        "Automatic thresholding + adjustments\n",
        "\n",
        "-   Reflections = impossible eye movements\n",
        "\n",
        "Pupil tracking\n",
        "\n",
        "-   Centroid versus Ellipse\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](_images/_session1/options.jpg){height=\"550px\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Camera set up\n",
        "\n",
        "-   Ask them to move their eyes around the corners of the screen\n",
        "    -   Notice whether we lose track at any place\n",
        "    -   Especially near your AIs\n",
        "    -   Ensure pupil is not covered\n",
        "        -   E.g., hair, glasses frame\n",
        "\n",
        "## Camera set up: Issues\n",
        "\n",
        "-   No mascara\n",
        "    -   Dark lashes lead to issues -\\> confounded with the pupil.\n",
        "-   Eyelids\n",
        "    -   Cover the pupil in lower gaze directions\n",
        "    -   An issue in some cases (e.g., tired participants)\n",
        "    -   Recording from lower then useful.\n",
        "\n",
        "## Camera set up: Issues\n",
        "\n",
        "-   Glasses\n",
        "    -   Darker image -\\> issues in thresholding\n",
        "    -   Clean glasses\n",
        "    -   Adjust contrast/brightness\n",
        "-   Glasses\n",
        "    -   Reflect infrared light\n",
        "    -   Clean glasses\n",
        "    -   Move camera (angle rather than distance)\n",
        "        -   Place reflections far away from pupil and CR\n",
        "    -   Move mirror (only possible in tower-mounted eye-trackers)\n",
        "\n",
        "## Camera set up: Issues\n",
        "\n",
        "-   Contact lenses\n",
        "    -   Air bubbles between eye and contact lense interact with CR\n",
        "        -   Camera focus\n",
        "-   Wet eyes\n",
        "    -   Split up the CR\n",
        "        -   Breaks\n",
        "        -   Stop the experiment\n",
        "        -   Dim the light in the lab\n",
        "        -   Tell the participant to go home"
      ],
      "id": "c1f73173"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}