<!DOCTYPE html>
<html lang="en"><head>
<script src="session3_files/libs/clipboard/clipboard.min.js"></script>
<script src="session3_files/libs/quarto-html/tabby.min.js"></script>
<script src="session3_files/libs/quarto-html/popper.min.js"></script>
<script src="session3_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="session3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session3_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="session3_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="session3_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="session3_files/libs/quarto-contrib/videojs/video.min.js"></script>
<link href="session3_files/libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <meta name="author" content="Baltais &amp; Badaya">
  <title>Reading</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="session3_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="session3_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="session3_files/libs/revealjs/dist/theme/quarto.css">
  <link href="session3_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="session3_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="session3_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="session3_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <style>
  .center-xy {
    margin: 0;
    position: absolute;
    top: 40%;
    left: 40%;
    -ms-transform: translateY(-50%), translateX(-50%);
    transform: translateY(-50%), translateX(-50%);
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Reading</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Baltais &amp; Badaya 
</div>
</div>
</div>

</section>
<section id="welcome-back" class="slide level2">
<h2>Welcome back!</h2>
<p>Any questions from yesterday?</p>
</section>
<section id="overview-of-today" class="slide level2">
<h2>Overview of today</h2>
<p>Eye-tracking during reading</p>
<ol type="1">
<li>Eye movements in reading</li>
<li>Reading paradigms</li>
<li>Linking hypothesis</li>
<li>Reading measures</li>
<li>Examples and confounds</li>
<li>Data pre-processing</li>
</ol>
</section>
<section>
<section id="eye-movements-in-reading" class="title-slide slide level1 center">
<h1>Eye movements in reading</h1>

</section>
<section id="reading-research-in-a-nutshell" class="slide level2">
<h2>Reading research in a nutshell</h2>
<p>We have a wealth of <strong>paradigms</strong> to explore how individuals comprehend a piece of written text (from single words to multiple sentences, created stimuli or real texts), considering both <strong>low-level processes</strong> for word identification (e.g., lexical access) and <strong>higher-level processes</strong> for sentence and discourse comprehension (e.g., inferences).</p>
</section>
<section id="reading-is-a-skill" class="slide level2">
<h2>Reading is a skill</h2>
<ul>
<li>Reading is relatively new
<ul>
<li>Homo Sapiens <span class="math inline">\(\pm\)</span> 300.000 BC</li>
<li>Sumerian pictographic writing <span class="math inline">\(\pm\)</span> 3.300 BC</li>
<li>Phoenician alphabetic language <span class="math inline">\(\pm\)</span> 2.000 BC</li>
</ul></li>
<li>Exposure, practice, formal instruction
<ul>
<li>We are good at it (<span class="math inline">\(\pm\)</span> 250 WPM; Brysbaert, 2019)</li>
<li>Linked to academic achievement, late L2 acquisition</li>
</ul></li>
</ul>
</section>
<section id="history" class="slide level2">
<h2>History</h2>
<ul>
<li><p>Javal (1879)</p>
<ul>
<li>Reading is not smooth</li>
</ul></li>
<li><p>Mid-70s: Technological advances</p></li>
<li><p>Rayner’s work</p>
<ul>
<li>Clifton et al., 2016: “Eye movements in reading and information processing: Keith Rayner’s 40 year legacy”</li>
</ul></li>
</ul>
<aside class="notes">
<p>In 1879, the French ophthalmologist Louis Émile Javal used a mirror on one side of a page to observe eye movement in silent reading, and found that it involves a succession of discontinuous individual movements for which he coined the term saccades.</p>
<p>Keith Rayner (June 20, 1943 – January 21, 2015) was a cognitive psychologist best known for pioneering modern eye-tracking methodology in reading and visual perception.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="some-general-facts" class="slide level2">
<h2>Some general facts</h2>
<ul>
<li>Most people are good at reading, but there are individual differences (e.g., reading proficiency, L1 vs.&nbsp;L2…)</li>
<li>There seems to be some immediacy<sup>1</sup> when comprehending a text, but we also get ahead of ourselves</li>
<li>In contrast to spoken language, people can go back when they encounter a difficulty, and indeed they do</li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn1"><p>Cf. linking hypothesis; serial vs.&nbsp;parallel processing debate.</p></li></ol></aside></section>
<section id="eye-movements-in-reading-1" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Human Visual System (recap):</p>
<ul>
<li><strong>Fovea</strong>: highest visual acuity</li>
<li><strong>Parafovea</strong>: still partial recognition of objects</li>
<li><strong>Periphery</strong>: blurred image</li>
</ul>
<p><strong>Fixations</strong> vs.&nbsp;<strong>saccades</strong>: movements (saccades) that place objects on the fovea for processing (fixations).</p>
</section>
<section id="eye-movements-in-reading-2" class="slide level2">
<h2>Eye movements in reading</h2>
<p>How does this work in reading?</p>
<ul>
<li>A priori: Our eyes move, so that objects (words?) are placed on the fovea and can be processed.</li>
</ul>
<p>…but this is not exactly so.</p>
</section>
<section id="eye-movements-in-reading-3" class="slide level2">
<h2>Eye movements in reading</h2>

<img data-src="_images/_session3/perceptual_span_reading.JPG" width="590" class="r-stretch quarto-figure-center"><p class="caption">From Conklin et al., 2018</p></section>
<section id="eye-movements-in-reading-4" class="slide level2">
<h2>Eye movements in reading</h2>
<ul>
<li>Fovea does not equate the perceptual span (= effective visual field)</li>
<li>Reading is asymmetric = asymmetric extraction of information
<ul>
<li>Bias towards the direction of reading in the language</li>
<li>English: 3-4<sup>1</sup> letter spaces (1 degree of visual angle) to the left, 14-15 ls to the right</li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn2"><p>Depends on print size, of course.</p></li></ol></aside></section>
<section id="eye-movements-in-reading-5" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Fixations</p>
<ul>
<li>Mean duration (silent reading in English): 200 - 250 ms</li>
<li>Lexical access initiated at about 100 ms (Sereno &amp; Rayner, 2003)</li>
<li>Optimal Viewing Position</li>
<li>Most reading measures are based on fixation durations</li>
<li>Affected by word length, frequency, predictability, etc.</li>
</ul>

<img data-src="_images/_session3/mariia_fix.JPG" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p><aside class="notes">
<p>The Optimal Viewing Position (OVP) effect shows that word identification is best when the eyes first fixate near the center of words. When words are presented for short durations, recognition performance is maximal when the viewing position is slightly left of the word center.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-movements-in-reading-6" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Saccades</p>
<ul>
<li>Saccadic supression</li>
<li>Saccadic latency (about 200 ms)</li>
<li>Mean size (silent reading in English): 7 to 9 letter spaces (<span class="math inline">\(\pm\)</span> 2 words before or after the fixated word)
<ul>
<li>Size = distance travelled (amplitude)</li>
</ul></li>
<li>Not analyzed except for backward saccades (regressions)</li>
</ul>

<img data-src="_images/_session3/mariia_sac.JPG" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p><aside class="notes">
<p>Saccadic suppression is a visual phenomenon where the brain temporarily reduces sensitivity to visual input during rapid eye movements (saccades). This suppression helps prevent a blurred, unstable visual experience when the eyes rapidly move across a scene.</p>
<p>Saccadic latency (150-250 ms): time needed to program and launch a saccade. This latency period includes the time required for the brain to process visual information, make a decision about where to move the eyes next, and initiate the eye movement.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-movements-in-reading-7" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Regressions</p>
<ul>
<li>10-15% of the saccades are regressions to preceding areas
<ul>
<li>To correct ‘overshooting’</li>
<li>Processing difficulty</li>
</ul></li>
</ul>

<img data-src="_images/_session3/mariia_sac.JPG" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p></section>
<section id="eye-movements-in-reading-8" class="slide level2">
<h2>Eye movements in reading</h2>

<img data-src="_images/_session3/skipping.JPG" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="eye-movements-in-reading-9" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Skipping</p>
<ul>
<li>70% of the words in a text are fixated</li>
<li>Content words are fixated 85% of the time, function words 35% of the time</li>
<li>Main predictor: word length; also predictability, frequency, etc.
<ul>
<li>Parafoveal processing!</li>
</ul></li>
<li>Due to ‘overshooting’ or processing ease</li>
</ul>
</section>
<section id="eye-movements-in-reading-10" class="slide level2">
<h2>Eye movements in reading</h2>
<p>Blinking</p>
<ul>
<li>Noise
<ul>
<li>Cf. data pre-processing</li>
</ul></li>
<li>Cognitive role (attention)?
<ul>
<li>Cornelis et al.&nbsp;(2025): naturalistic reading</li>
<li>More blinks at punctuation marks; less blinks if high word frequency and predictability (“time-out”?)</li>
</ul></li>
</ul>

<img data-src="_images/_session3/mariia_blink.JPG" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p></section></section>
<section>
<section id="reading-paradigms" class="title-slide slide level1 center">
<h1>Reading paradigms</h1>

</section>
<section id="reading-paradigms-1" class="slide level2">
<h2>Reading paradigms</h2>
<p>Differ in ecological validity.</p>
<ul>
<li>Gaze-contigent paradigms: eye gaze determines changes in the text display</li>
<li>Reading of experimental stimuli</li>
<li>Reading of natural text</li>
</ul>
</section>
<section id="gaze-contigent-paradigms" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Moving window paradigm (e.g., McConkie &amp; Rayner, 1975)</p>
<ul>
<li>IA<sup>1</sup>: Mask everything that’s not fixated.</li>
<li>Shows: Parafoveal processing and perceptual span (extraction and use of information).</li>
<li>IV: Size of the window.</li>
</ul>
<aside class="notes">
<p>By varying the size of the window and the type of mask (e.g., X’s, visually similar or dissimilar characters) and comparing the reading times in the window and normal reading conditions, it is possible to define the size of the area from which a reader can efficiently extract and utilize information.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><ol class="aside-footnotes"><li id="fn3"><p>This paper was retracted due to some legal issues.</p></li></ol></aside></section>
<section id="gaze-contigent-paradigms-1" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Moving window paradigm (e.g., McConkie &amp; Rayner, 1975)</p>
<p><br></p>
<video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="_images/_session3/moving_window.mp4"></video>
</section>
<section id="gaze-contigent-paradigms-2" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Some other examples (see next slides for details).</p>
<ul>
<li>Foveal mask (moving mask) paradigm: the reverse of moving window</li>
<li>Disappearing text paradigm</li>
<li>Fast priming paradigm</li>
<li>Boundary paradigm</li>
<li>etc.</li>
</ul>
</section>
<section id="gaze-contigent-paradigms-3" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Foveal mask paradigm (e.g., Rayner &amp; Bertera, 1979)</p>
<ul>
<li>IA: Mask what’s fixated.</li>
<li>Shows: Parafoveal processing and perceptual span.</li>
<li>IV: Size of the window.</li>
</ul>
</section>
<section id="gaze-contigent-paradigms-4" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Foveal mask paradigm (e.g., Rayner &amp; Bertera, 1979)</p>
<p><br></p>
<video id="video_shortcode_videojs_video2" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="_images/_session3/foveal_mask.mp4"></video>
</section>
<section id="gaze-contigent-paradigms-5" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Disappearing text paradigm (e.g., Liversedge et al., 2004)</p>
<ul>
<li>IA: Disappears after having been fixated for a certain time.</li>
<li>Shows: Parafoveal + foveal processing.</li>
<li>IV: Time to disappear.
<ul>
<li>Amount of visual exposure necessary for word recognition.</li>
</ul></li>
</ul>
</section>
<section id="gaze-contigent-paradigms-6" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Disappearing text paradigm (e.g., Liversedge et al., 2004)</p>
<p><br></p>
<video id="video_shortcode_videojs_video3" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="_images/_session3/disappearing_text_animation.mp4"></video>
</section>
<section id="gaze-contigent-paradigms-7" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Fast priming paradigm (e.g., Sereno &amp; Rayner, 1992)</p>
<ul>
<li>IA: First there is a prime in the target location. After the target location has been fixated for some time, the target word appears.</li>
<li>Shows: Priming effect (facilitation).</li>
<li>IV: Relationship prime and target.</li>
</ul>
</section>
<section id="gaze-contigent-paradigms-8" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Fast priming paradigm (e.g., Sereno &amp; Rayner, 1992)</p>
<p><br></p>
<video id="video_shortcode_videojs_video4" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="_images/_session3/fast_priming_paradigm.mp4"></video>
</section>
<section id="gaze-contigent-paradigms-9" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Boundary paradigm (e.g., Rayner, 1975)</p>
<ul>
<li>Saccadic suppression.</li>
<li>IA: Masked and then changed to target word after the gaze crosses an invisible boundary.</li>
<li>Shows: Parafoveal processing and perceptual span.
<ul>
<li>Preview effect (slowdown).</li>
<li>Preview benefit when no mask in the control condition.</li>
<li>Parafoveal-on-foveal effects (e.g., Drieghe, 2011).</li>
</ul></li>
<li>IV: What information is extracted, e.g., visual similarity mask-target.</li>
</ul>
<aside class="notes">
<p>The boundary paradigm (Rayner, 1975) makes use of the saccadic suppression. Saccadic suppression means that during a saccade the intake of visual information is suspended and the reader is practically blind. If a change in the visual environment is made during a saccade or very soon after the eyes have landed (&lt; 6 ms after the end of a saccade, McConkie &amp; Loschky, 2002), the reader does not become consciously aware of it. The target word (“sentence” in the example of Table 7.1) is initially masked with a character string (“somkasoc”), and when the reader’s eyes cross an invisible boundary in the text, the mask is replaced with the actual target word.</p>
<p>If the reader has extracted information from the target word preview prior to its change to the correct form, one should observe increased fixation time on the target word, even though the reader is not consciously aware of this. The size of the slowdown in eye fixation time, i.e.&nbsp;the difference between normal condition in which no change was made and a change condition is called the preview effect.</p>
<p>The preview benefit is simply computed as the difference in fixation time between a full preview condition, in which the target word was presented normally, and the preview condition.</p>
<p>Another measure to assess parafoveal processing is the so-called parafoveal-on-foveal effect (Kennedy, 2000). It measures the extent to which parafoveally available information affects fixation time on the previous word (see review by Drieghe, 2011).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="gaze-contigent-paradigms-10" class="slide level2">
<h2>Gaze-contigent paradigms</h2>
<p>Boundary paradigm (e.g., Rayner, 1975)</p>
<p><br></p>
<video id="video_shortcode_videojs_video5" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="_images/_session3/boundary.mp4"></video>
</section>
<section id="reading-of-experimental-stimuli" class="slide level2">
<h2>Reading of experimental stimuli</h2>
<p>Linguistic and non-linguistic factors affecting eye movement patterns in reading.</p>
<p>Usually, exploration of how patterns over an IA differ as a function of its properties.</p>
</section>
<section id="reading-of-experimental-stimuli-sentences" class="slide level2">
<h2>Reading of experimental stimuli (sentences)</h2>
<p>Frazier &amp; Rayner, 1982</p>
<ul>
<li>Garden-path sentences: early closure vs.&nbsp;late closure.</li>
</ul>

<img data-src="_images/_session3/fraz.JPG" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">

<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="reading-of-experimental-stimuli-texts" class="slide level2">
<h2>Reading of experimental stimuli (texts)</h2>
<p>Pellicer-Sánchez, 2016</p>
<ul>
<li>Incidental L2 vocabulary acquisition over repetitive exposure</li>
</ul>

<img data-src="_images/_session3/texts_example.JPG" class="r-stretch quarto-figure-center"><p class="caption">From Conklin et al., 2018</p></section>
<section id="reading-of-natural-text" class="slide level2">
<h2>Reading of natural text</h2>
<p>GECO corpus (Cop et al., 2017)</p>

<img data-src="_images/_session3/geco.JPG" class="r-stretch"><ul>
<li>The Mysterious Affair At Styles by Agatha Christie (1920)</li>
</ul>
</section>
<section id="reading-of-natural-text-1" class="slide level2">
<h2>Reading of natural text</h2>
<p>Ecological validity but lack of control over text:</p>
<ul>
<li>Data collection: participants’ characteristics, matching samples</li>
<li>Analysis: mixed-effects models, control variables (already annotated in corpora)</li>
</ul>
</section></section>
<section>
<section id="linking-hypothesis" class="title-slide slide level1 center">
<h1>Linking hypothesis</h1>

</section>
<section id="linking-hypothesis-1" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Fixations, regressions, skipping -&gt; wide range of measures.</p>
<p>Reading (during a fixation) entails the use of visual, orthographic, phonological, and morphological information to:</p>
<ul>
<li>Identify a word</li>
<li>Activate its lexical representation</li>
<li>Integrate with the preceding context</li>
<li>Predict what’s next?</li>
</ul>
</section>
<section id="linking-hypothesis-2" class="slide level2">
<h2>Linking hypothesis</h2>
<ul>
<li><p>What we look at = what we are processing</p></li>
<li><p>For how long we look = how difficult it is to process</p>
<ul>
<li>Extends beyond linguistic processing (visual attention)</li>
</ul></li>
</ul>
<p>Just and Carpenter’s (1980) eye-mind hypothesis:</p>
<ul>
<li>The eye remains fixated on a word as long as the word is being processed</li>
</ul>
<p>Just and Carpenter’s (1980) immediacy hypothesis:</p>
<ul>
<li>Readers try to interpret each content word as soon as it is encountered</li>
</ul>
<aside class="notes">
<p>From J&amp;C:</p>
<p>Gaze durations reflect the time to execute comprehension processes. Longer fixations are attributed to longer processing caused by the word’s infrequency and its thematic importance.</p>
<p>The model of reading comprehension proposes that readers interpret a word while they are fixating it, and they continue to fixate it until they have processed it as far as they can.</p>
<p>The link between eye fixation data and the theory rests on two assumptions. One is the eye-mind assumption: <strong>the eye remains fixated on a word as long as the word is being processed</strong>. The eye-mind assumption posits that there is no appreciable lag between what is being fixated and what is being processed.</p>
<p>The immediacy assumption: a reader tries to interpret each content word of a text as it is encountered, even at the expense of making guesses that sometimes turn out to be wrong. The immediacy assumption posits that the interpretations at all levels of processing are not deferred; they occur as soon as possible.</p>
<p>The immediacy assumption posits that an attempt to relate each content word to its referent occurs as soon as possible. Sometimes this can be done when the word is first fixated, but sometimes more information is required. For example, although the semantic interpretation of a relative adjective like large can be computed immediately, the extensive meaning depends on the word it modifies (e.g., large insect vs.&nbsp;large building). The referent of the entire noun phrase can be computed only after both words are processed. The immediacy assumption does not state that the relating is done immediately on each content word, but rather that it occurs as soon as possible.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linking-hypothesis-3" class="slide level2">
<h2>Linking hypothesis</h2>
<p>The more difficult the text…</p>
<ul>
<li>The ________________ the fixation durations</li>
<li>The ________________ the saccade sizes</li>
<li>The ________________ the regressions</li>
<li>The ________________ the skipping of words</li>
</ul>
</section>
<section id="linking-hypothesis-4" class="slide level2">
<h2>Linking hypothesis</h2>
<p>The more difficult the text…</p>
<ul>
<li>The longer the fixation durations</li>
<li>The smaller the saccade sizes</li>
<li>The more frequent the regressions</li>
<li>The less frequent the skipping of words</li>
</ul>
</section>
<section id="linking-hypothesis-5" class="slide level2">
<h2>Linking hypothesis</h2>
<ul>
<li><p>“Strong” eye-mind hypothesis: readers fixate a word until it’s processed as far as possible (how far?)</p>
<ul>
<li><p>Morrison, 1984: completion of lexical access on word <em>n</em> -&gt; immediate shift in attention &amp; planning a saccade to word <em>n+1</em></p></li>
<li><p>Boland, 2004: the eyes do not leave a word until it has been <em>structurally</em> integrated</p></li>
</ul></li>
</ul>
<aside class="notes">
<p>Boland, 2004: The eyes do not leave a word until it has been structurally integrated. Therefore, <strong>constraints that control structure-building influence first-pass reading time</strong>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linking-hypothesis-6" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Too strong?</p>
<ul>
<li><p>Pickering et al.&nbsp;(2004): some aspects of processing take more time than the eye is “prepared to wait”</p>
<ul>
<li>Semantic processing: “A lot of Americans protested during Finland.”</li>
</ul></li>
<li><p>Mitchell et al.&nbsp;(2008): regressions are not only linguistically supervised (‘time-out’ to postpone new input)</p></li>
</ul>
<aside class="notes">
<p>Pickering et al.&nbsp;(2004): some aspects of lexical, syntactic, and semantic processing do (largely) respect the immediacy and eye-mind assumptions (with some important caveats), but that <strong>many aspects of sentence interpretation are somewhat delayed</strong>. Semantic processing may not all occur “at once” (readers do look at difficult words such as Finland - in “A lot of Americans protested during Finland”, - but they largely do this during later processing rather than during first pass).</p>
<p>Mitchell et al.&nbsp;(2008): the purpose of regressive fixations (and, indeed, re-fixations on the same word) is not to refresh the evidence but merely a delaying tactic used to provide “time out” for as-yet-incomplete parsing operations (to postpone new input). Linguistic and other forms of eye-movement control are rather loosely-, not tightly-coupled.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linking-hypothesis-7" class="slide level2">
<h2>Linking hypothesis</h2>
<p>Asymmetrical perceptual span:</p>
<ul>
<li><strong>Preview benefit</strong>: some info about <em>n+1</em> while still on <em>n</em> -&gt; shorter reading times <em>n+1</em>
<ul>
<li>Lexical processing can start before a word is fixated</li>
</ul></li>
<li><strong>Parafoveal-on-foveal effects</strong>: characteristics of <em>n+1</em> influence reading times for <em>n</em>
<ul>
<li>Mixed findings, see Table 4.1 in Conklin et al., 2018 (p.&nbsp;83)</li>
</ul></li>
</ul>
</section>
<section id="linking-hypothesis-8" class="slide level2">
<h2>Linking hypothesis</h2>
<p>As well as:</p>
<ul>
<li><strong>Spillover effects</strong>: characteristics of <em>n</em> influence reading times for <em>n+1</em>
<ul>
<li>Processing has not been completed</li>
</ul></li>
</ul>
<p>Therefore,</p>
<ul>
<li><p>Serial vs.&nbsp;parallel processing debate</p>
<ul>
<li><p>Different models of reading (E-Z Reader vs.&nbsp;SWIFT; OB1-Reader)</p></li>
<li><p>Mlinarič et al.&nbsp;(2025): ± three words in parallel</p></li>
</ul></li>
</ul>
<aside class="notes">
<p>Serial: words are processed one at a time in a linear sequence. E-Z: eee-zee (easy).</p>
<p>Parallel: words are processed simultaneously. The cognitive system processes information from several words at once, allowing for faster and more efficient reading. Attention is distributed dynamically across several words, with a gradient of attention decreasing from the point of fixation outward. SWIFT: Saccade-Generation with Inhibition by Foveal Targets. The word currently fixated (the foveal target) inhibits the generation of saccades. This inhibition ensures that the word is sufficiently processed before the eyes move to the next word. Once a threshold of processing is reached, the inhibition decreases, allowing for the initiation of the next saccade. The SWIFT model emphasizes parallel processing and dynamic attention distribution, while the E-Z Reader model focuses on serial processing with some degree of parafoveal preview.</p>
<p>The OB1-Reader model (upgrade of E-Z?) incorporates both serial and parallel processing components: Optimal Viewing Position-Based Reading.</p>
<p>Mlinarič et al.&nbsp;(2025): ± three words in parallel, which is more than what some theories (e.g., E-Z Reader, Chinese Reading Model) suggest, but less than what some other theories (e.g., Glenmore, OB1-reader) suggest.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="reading-measures" class="title-slide slide level1 center">
<h1>Reading measures</h1>

</section>
<section id="reading-measures-1" class="slide level2">
<h2>Reading measures</h2>
<p>Fixations, regressions, and skipping form multiple reading measures that arguably tap into different cognitive processes.</p>
<ul>
<li><strong>Global</strong> vs.&nbsp;<strong>local</strong> measures
<ul>
<li>Overall reading behaviour vs.&nbsp;smaller units of text</li>
</ul></li>
<li>Local measures: <strong>early</strong> (vs.&nbsp;<strong>intermediate</strong>) vs.&nbsp;<strong>late</strong> (see also Clifton et al., 2007)</li>
</ul>
</section>
<section id="reading-measures-2" class="slide level2">
<h2>Reading measures</h2>
<p>Reading measures &lt;-&gt; specific cognitive events?</p>
<p>No agreement on <strong>linking</strong>: it depends on researchers’ theoretical assumptions (Boland, 2004).</p>
<p>Traditionally,</p>
<ul>
<li><strong>Early measures</strong>: highly automatic word <em>recognition</em> and lexical access processes</li>
<li><strong>Late measures</strong>: more conscious, controlled, strategic processes, such as syntactic and semantic <em>integration</em></li>
</ul>

<aside><div>
<p>Different types of linguistic manipulations (e.g., lexical vs.&nbsp;syntactic) can produce effects in the same measure –&gt; report both early and late measures.</p>
</div></aside></section>
<section id="early-measures" class="slide level2">
<h2>Early measures</h2>
<p>First-pass measures (before moving to <em>n+1</em>).</p>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li><strong>First fixation duration</strong> (ms)</li>
<li><strong>Single fixation duration</strong> (ms)</li>
<li><strong>Gaze duration</strong> (= first-pass reading time = first-run dwell time): time on <em>n</em> before leaving it in either direction</li>
<li><strong>Skipping rate</strong>: % trials where the IA was skipped on first pass</li>
<li>Influenced by length, frequency, predictability, etc.</li>
</ul>
<aside class="notes">
<p>From Clifton et al., 2007. Skipping: length, predictability. FFD: earliest point to observe an effect due to properties of a word, e.g., frequency. GD: sensitive to semantic and syntactic anomalies.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="early-measures-1" class="slide level2">
<h2>Early measures</h2>
<p>First-pass measures (before moving to <em>n+1</em>).</p>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>First fixation duration: 3</li>
<li>Single fixation duration: NA (two first-pass fix)</li>
<li>Gaze duration: 3+4</li>
<li>Skipping: no (IA wasn’t skipped in this trial)</li>
</ul>
</section>
<section id="intermediate-measures" class="slide level2">
<h2>Intermediate measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li><strong>Regression-path duration</strong> (= go-past time): time on <em>n</em>, <em>n-1</em>, etc., before moving to <em>n+1</em></li>
<li><strong>(First-pass) regressions-out rate</strong>: % trials where there was a regression out of the IA</li>
<li>Lexical and integration difficulties</li>
</ul>
<aside class="notes">
<p>From Clifton et al., 2007. RPD: lexical and integration difficulties. Regr-out: difficulties in higher-level processing (e.g., discourse processing) of a text.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="intermediate-measures-1" class="slide level2">
<h2>Intermediate measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>Regression-path duration: 3+4+5+6</li>
<li>First-pass regressions-out: yes (between fix 4 and 5)</li>
</ul>
</section>
<section id="late-measures" class="slide level2">
<h2>Late measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li><strong>Re-reading time, second-pass reading time</strong>: different definitions across literature</li>
<li><strong>Regressions-in rate</strong>: % trials where there was a regression back into the IA
<ul>
<li>Re-analysis, integration difficulties</li>
</ul></li>
<li><strong>Total reading time</strong> (= dwell time): sum of all fixations</li>
<li><strong>Fixation count</strong>
<ul>
<li>Could be early and/or late processing effects</li>
</ul></li>
</ul>
<aside class="notes">
<p>Re-reading time (= regression-path duration - gaze duration). Second-pass reading time (= total reading time - gaze duration).</p>
<p>From Clifton et al., 2007. Re-reading time (RPD-GD): indication of the time spent resolving a problem. TRT: impacted by both early and late processing. Effect in TRT but not in early measures -&gt; late effect on processing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="late-measures-1" class="slide level2">
<h2>Late measures</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/earlymeasures.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>Re-reading time (= regression-path duration - gaze duration): 5+6</li>
<li>Second-pass reading time (= total reading time - gaze duration): 6+9</li>
<li>Regressions-in: yes (between fix 8 and 9)</li>
<li>Total reading time: 3+4+6+9</li>
<li>Fixation count: four fix</li>
</ul>
</section>
<section id="reading-measures-3" class="slide level2">
<h2>Reading measures</h2>
<p>Measures are dependent on each other:</p>
<ul>
<li>First fixation duration <span class="math inline">\(\subseteq\)</span> gaze duration <span class="math inline">\(\subseteq\)</span> total reading time, etc.</li>
<li>Late measures are cumulative</li>
<li>No transparent mapping of measures to cognitive events!</li>
</ul>
<p>Therefore,</p>
<ul>
<li>An effect must be visible in &gt; 1 measure (rule of thumb)</li>
<li>Analysis: correction for multiple comparisons (Type I error)
<ul>
<li>cf.&nbsp;von der Malsburg &amp; Angele (2017)</li>
</ul></li>
</ul>
</section>
<section id="reading-measures-4" class="slide level2">
<h2>Reading measures</h2>
<p>Recap:</p>

<img data-src="_images/_session3/reiterate.JPG" class="r-stretch quarto-figure-center"><p class="caption">From Carroll, 2017</p></section>
<section id="reading-measures-exercise" class="slide level2 smaller">
<h2>Reading measures: exercise</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/exercise.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>First fixation duration:</li>
<li>Gaze duration:</li>
<li>First-pass regressions-out:</li>
<li>Regression-path duration:</li>
<li>Total reading time:</li>
<li>Fixation count:</li>
</ul>
<aside class="notes">
<p>3, 3, yes(two), 3+4+5+6+7, 3+5+7, three</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="reading-measures-exercise-1" class="slide level2 smaller">
<h2>Reading measures: exercise</h2>
<div class="{r-stack}">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/exercise.jpg" class="quarto-figure quarto-figure-center" width="900"></p>
</figure>
</div>
</div>
<ul>
<li>First fixation duration: 3</li>
<li>Gaze duration: 3</li>
<li>First-pass regressions-out: yes (between fix 3 and 4, fix 5 and 6)</li>
<li>Regression-path duration: 3+4+5+6+7</li>
<li>Total reading time: 3+5+7</li>
<li>Fixation count: three fix</li>
</ul>
</section>
<section id="reading-measures-minimum-set" class="slide level2">
<h2>Reading measures: minimum set</h2>
<p>Both early and late.</p>
<p>Usually reported:</p>
<ul>
<li>Skipping rate</li>
<li>First fixation duration</li>
<li>Gaze duration</li>
<li>First-pass regressions-out</li>
<li>Regression-path duration</li>
<li>Total reading time</li>
</ul>
<p>But: the more precise your hypotheses, the better! (fewer comparisons, cf.&nbsp;von der Malsburg &amp; Angele, 2017)</p>
</section></section>
<section>
<section id="examples-and-confounds" class="title-slide slide level1 center">
<h1>Examples and confounds</h1>

</section>
<section id="sentence-reading-experiment" class="slide level2">
<h2>Sentence reading experiment</h2>
<p>Elements:</p>
<ul>
<li>Text (e.g., a sentence)</li>
<li>Optional task (e.g., a comprehension question)</li>
</ul>
<p>Trial sequence: Drift correction -&gt; Visual presentation.</p>
</section>
<section id="drift-correction" class="slide level2">
<h2>Drift correction</h2>
<p>Where the first character appears:</p>

<img data-src="_images/_session3/drift_reading.jpg" class="quarto-figure quarto-figure-center r-stretch" width="400"></section>
<section id="visual-presentation" class="slide level2">
<h2>Visual presentation</h2>
<ul>
<li>Time window of interest is from start to end (no triggers in between)
<ul>
<li>Cf. gaze-contingent paradigms</li>
</ul></li>
</ul>
<p>Instead, Interest Areas:</p>

<img data-src="_images/_session3/ia_reading.png" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p></section>
<section id="interest-areas" class="slide level2">
<h2>Interest Areas</h2>
<ul>
<li>Target region <em>n</em> but also
<ul>
<li>Pre-target region <em>n-1</em></li>
<li>Post-target (spillover) region <em>n+1</em></li>
</ul></li>
</ul>
<p>Size depends on the research question:</p>
<ul>
<li>Single words, phrases, entire clauses…</li>
</ul>
</section>
<section id="interest-areas-1" class="slide level2">
<h2>Interest Areas</h2>
<ul>
<li>Vertically large enough
<ul>
<li>Pre-processing: vertical drift correction</li>
</ul></li>
</ul>

<img data-src="_images/_session3/mariia_verticaldrift.png" width="500" class="r-stretch quarto-figure-center"><p class="caption">Data from Mariia Baltais</p></section>
<section id="task" class="slide level2">
<h2>Task</h2>
<ul>
<li><p>Move to the next trial (button press)</p>
<ul>
<li><p>Consider handedness (e.g, turn the button box over)</p></li>
<li><p>Useful trick: before pressing, look at the right lower corner of the monitor (e.g., a sticker)</p></li>
</ul></li>
<li><p>Comprehension questions</p>
<ul>
<li>Every now and then / half of the trials / every trial…</li>
</ul></li>
</ul>
</section>
<section id="confounds" class="slide level2">
<h2>Confounds</h2>
<ul>
<li>Non-linguistic
<ul>
<li>Visual presentation, eye-tracker specifics, human visual system</li>
</ul></li>
<li>Linguistic
<ul>
<li>Characteristics of the materials</li>
</ul></li>
</ul>
</section>
<section id="non-linguistic-confounds" class="slide level2">
<h2>Non-linguistic confounds</h2>
<p>Text layout:</p>
<ul>
<li>Font style and size
<ul>
<li>B&amp;W is not recommended for dyslexic participants</li>
<li>Monospace font (e.g., Courier)</li>
<li>14-18 pt<sup>1</sup> (3 letters = 1 degree)</li>
</ul></li>
<li>Margins
<ul>
<li>Tracking less accurate at screen borders</li>
</ul></li>
</ul>
<aside class="notes">
<p>Conklin et al., 2018: a study with dyslexic participants might use pastel colours for the background to help reduce contrast levels.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><ol class="aside-footnotes"><li id="fn4"><p>Depends on the distance to monitor.</p></li></ol></aside></section>
<section id="non-linguistic-confounds-1" class="slide level2">
<h2>Non-linguistic confounds</h2>
<p>Target (&amp; spillover) IA:</p>
<ul>
<li>Never beginning/end of the line
<ul>
<li>1<span class="math inline">\(^{st}\)</span> word likely to be under-/overshoot or skipped</li>
<li>1<span class="math inline">\(^{st}\)</span> and last fixations on a line are 5-7 ls from the edges</li>
<li>1<span class="math inline">\(^{st}\)</span> fix longer, last fix shorter</li>
</ul></li>
<li>Never clause-final
<ul>
<li>Wrap-up effect</li>
</ul></li>
</ul>
<aside class="notes">
<p>Conklin et al., 2018: Aside from the problem of sentence wrap-up effects, fixations are not always stable when readers move from one line to the next, and often there is a tendency to under- or overshoot the target (the first word of the next line). Minor corrective saccades would influence reading times.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="non-linguistic-confounds-2" class="slide level2">
<h2>Non-linguistic confounds</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Line spacing<sup>1</sup>
<ul>
<li>Tracking less accurate for vertical movements (vertical drift)</li>
<li>Double/triple spacing</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/example_spacing.JPG"></p>
<figcaption>From Conklin et al., 2018</figcaption>
</figure>
</div>
</div></div>
<aside><ol class="aside-footnotes"><li id="fn5"><p>Less important for global measures (sentence-level processing).</p></li></ol></aside></section>
<section id="linguistic-confounds" class="slide level2">
<h2>Linguistic confounds</h2>
<p>See Clifton et al., 2016; Conklin et al., 2018 (Ch. 4):</p>
<ul>
<li><strong>The Big Three</strong> (Frequency, Length, Predictability)</li>
</ul>
<p>As well as…</p>
<ul>
<li>Familiarity, plausibility, prevalence, age of acquisition, lexical ambiguity, orthographic neighbourhood size, etc.</li>
</ul>
<aside class="notes">
<p>Plausibility (real world knowledge). Prevalence (word knowledge in the population). Lexical ambiguity (number of word meanings). Orthographic neighbourhood size (neighbours differ by 1 letter).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linguistic-confounds-1" class="slide level2">
<h2>Linguistic confounds</h2>
<p>Foveal vs.&nbsp;parafoveal processing:</p>
<ul>
<li><p>Sequential nature of reading.</p></li>
<li><p>Information from one area can influence eye movements in another area.</p></li>
</ul>
<p>Match contexts!</p>
<ul>
<li>Predictability, length, etc.</li>
<li>Serial vs.&nbsp;parallel processing:
<ul>
<li>Preview benefit, parafoveal-on-foveal effects (see Drieghe, 2011; Hyönä, 2011)</li>
<li>Spillover effects</li>
</ul></li>
</ul>
</section>
<section id="example-juhasz-sheridan-2020" class="slide level2">
<h2>Example: Juhasz &amp; Sheridan (2020)</h2>
<p>“The time course of age-of-acquisition effects on eye movements during reading: Evidence from survival analyses”</p>
<p>Background:</p>
<ul>
<li>AoA effect: faster processing for words that are learned earlier in life.</li>
<li>Network plasticity hypothesis: early learned items are encoded more effectively -&gt; AoA influences multiple aspects of lexical processing, including orthography, phonology, and semantics.</li>
<li>Alternatively, AoA effect is limited to semantic processing.</li>
</ul>
<p>RQ: Does AoA influence the earliest stages of word recognition during reading?</p>
</section>
<section id="example-juhasz-sheridan-2020-1" class="slide level2">
<h2>Example: Juhasz &amp; Sheridan (2020)</h2>
<p>Linking hypothesis: E-Z Reader model</p>
<ul>
<li>Lexical processing is serial</li>
<li>Direct lexical control over eye movements</li>
<li>Two stages: 1) orthography, phonology, familiarity (-&gt; saccade planning), 2) semantics (-&gt; shift in attention)</li>
</ul>
<p><em>Which reading measures are of most interest?</em></p>
</section>
<section id="example-juhasz-sheridan-2020-2" class="slide level2">
<h2>Example: Juhasz &amp; Sheridan (2020)</h2>
<p>Lexical access -&gt; early measures.</p>
<ul>
<li>First fixation duration (!), single fixation duration, gaze duration</li>
<li>Skipping rate</li>
<li>Total reading time</li>
</ul>
<p>Plus a time-course analysis to estimate the earliest divergence point.</p>
</section>
<section id="example-juhasz-sheridan-2020-3" class="slide level2">
<h2>Example: Juhasz &amp; Sheridan (2020)</h2>
<p>Early AoA: Julia will go to the mall to buy a new <strong>skirt</strong> when she gets out of work.</p>
<p>Late AoA: Julia will go to the mall to buy a new <strong>shawl</strong> when she gets out of work.</p>
<p><em>Which potential confounds can you think of?</em></p>
</section>
<section id="example-juhasz-sheridan-2020-4" class="slide level2">
<h2>Example: Juhasz &amp; Sheridan (2020)</h2>
<p>Early AoA: Julia will go to the mall to buy a new <strong>skirt</strong> when she gets out of work.</p>
<p>Late AoA: Julia will go to the mall to buy a new <strong>shawl</strong> when she gets out of work.</p>
<ul>
<li>Same context through at least the post-target word</li>
<li>Target word never the first or last two words</li>
</ul>
</section>
<section id="example-juhasz-sheridan-2020-5" class="slide level2">
<h2>Example: Juhasz &amp; Sheridan (2020)</h2>

<img data-src="_images/_session3/juhasz.png" class="quarto-figure quarto-figure-center r-stretch" width="400"><aside class="notes">
<p>Existing databases and norming studies.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example-juhasz-sheridan-2020-6" class="slide level2">
<h2>Example: Juhasz &amp; Sheridan (2020)</h2>
<p>Finding:</p>
<ul>
<li>AoA effects were observed in both early (at 158 ms) and late fixation duration measures.</li>
</ul>
<p><em>What did we learn about lexical processing in reading?</em></p>
</section>
<section id="example-juhasz-sheridan-2020-7" class="slide level2">
<h2>Example: Juhasz &amp; Sheridan (2020)</h2>
<ul>
<li>Word properties can directly influence fixation durations from early on</li>
<li>AoA affects multiple measures (-&gt; multiple loci within the mental lexicon)</li>
<li>Consistent with SWIFT, too (lexical processing difficulty inhibits saccade programming)</li>
<li>Match contexts!</li>
</ul>
</section>
<section id="example-howard-et-al.-2017" class="slide level2">
<h2>Example: Howard et al.&nbsp;(2017)</h2>
<p>“Processing of Co-Reference in Autism Spectrum Disorder”</p>
<p>Background:</p>
<ul>
<li>Typicality effect: longer reading times for a category noun (“bird”) when preceded by an atypical exemplar (“penguin”)</li>
<li>ASD: reduced accuracy for reading comprehension and inferencing tasks (e.g., anaphor -&gt; antecedent)</li>
</ul>
<p>RQ: Are individuals with ASD less efficient when computing co-referential links during reading?</p>
<p><em>Which reading measures are of most interest?</em></p>
</section>
<section id="example-howard-et-al.-2017-1" class="slide level2">
<h2>Example: Howard et al.&nbsp;(2017)</h2>
<p>Comprehension -&gt; global measures:</p>
<ul>
<li>Mean fixation duration</li>
<li>Fixation count</li>
<li>Total reading time</li>
</ul>
<p>Inferencing -&gt; target word (“bird”):</p>
<ul>
<li>First fixation duration</li>
<li>Single fixation duration</li>
<li>Gaze duration</li>
<li>Total reading time</li>
</ul>
</section>
<section id="example-howard-et-al.-2017-2" class="slide level2">
<h2>Example: Howard et al.&nbsp;(2017)</h2>
<p>The instruction booklet said that Tom would need a <em>hammer/plunger</em> to fix the kitchen sink. He borrowed the <strong>tool</strong> from his next door neighbour.</p>
<p><em>How would you define the IAs?</em></p>
</section>
<section id="example-howard-et-al.-2017-3" class="slide level2">
<h2>Example: Howard et al.&nbsp;(2017)</h2>

<img data-src="_images/_session3/ias_howard.png" class="quarto-figure quarto-figure-center r-stretch"><p><em>Which potential confounds can you think of?</em></p>
</section>
<section id="example-howard-et-al.-2017-4" class="slide level2">
<h2>Example: Howard et al.&nbsp;(2017)</h2>
<ul>
<li>Target words &amp; contexts are identical across conditions</li>
<li>Two lines, target in the middle of the second line</li>
<li>Norming study for typicality</li>
<li>Individual differences!</li>
</ul>

<img data-src="_images/_session3/indvar_howard.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<p>Plunger: to unclog the WC.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example-howard-et-al.-2017-5" class="slide level2">
<h2>Example: Howard et al.&nbsp;(2017)</h2>
<p>Finding:</p>
<ul>
<li>No group differences (except for more re-reading in ASD)</li>
<li>ASD difficulties in reading comprehension are unlikely to be accounted for by differences in co-reference processing</li>
<li>Effects of verbal proficiency (expressive language score) across groups</li>
</ul>
<p><em>What to consider when building your own experiment?</em></p>
</section>
<section id="exercise-wooclap" class="slide level2">
<h2>Exercise: Wooclap</h2>
<p>Think of a research question that can be answered using an eye-tracking during reading paradigm.</p>
<ul>
<li>Which eye-tracking measures would you look at?</li>
<li>Which potential confounds would you consider?</li>
</ul>
<p>Go to Wooclap (MILS25ETD3) and type your idea!</p>
</section>
<section id="exercise-wooclap-1" class="slide level2">
<h2>Exercise: Wooclap</h2>

<img data-src="_images/_session3/wooclap_D3.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="more-examples-research-topics" class="slide level2">
<h2>More examples: research topics</h2>
<ul>
<li>Word recognition
<ul>
<li>Frequency effect (Rayner &amp; Duffy, 1986)</li>
</ul></li>
<li>Syntactic processing
<ul>
<li>Structural ambiguities (Frazier &amp; Rayner, 1982)</li>
</ul></li>
<li>Semantic integration
<ul>
<li>Plausibility effect (Staub et al., 2007)</li>
</ul></li>
<li>Predictive processing (Frisson et al., 2017)</li>
<li>Parafoveal processing (Juhasz et al., 2009)</li>
<li>Text comprehension (Dirix et al., 2019)</li>
<li>Multiword units (Carrol et al., 2016)</li>
<li>Skim reading (Jayes et al., 2022)</li>
</ul>
</section>
<section id="more-examples-populations" class="slide level2">
<h2>More examples: populations</h2>
<ul>
<li>L2 speakers
<ul>
<li>Gender agreement (Keating, 2009)</li>
<li>Pronoun resolution (Puebla &amp; Felser, 2022)</li>
</ul></li>
<li>Children (Blythe et al., 2011)</li>
<li>Older adults (Solan et al., 1995)</li>
<li>Clinical populations
<ul>
<li>Children with dyslexia (Hyönä et al., 1995)</li>
<li>Adults with ASD (Howard et al., 2017)</li>
</ul></li>
</ul>
</section>
<section id="what-to-report" class="slide level2">
<h2>What to report?</h2>
<p>Materials:</p>
<ul>
<li>Paradigm &amp; stimuli (how were the Interest Areas defined?)</li>
<li>What potential confounds were controlled for (e.g., the Big Three)?</li>
</ul>
</section>
<section id="what-to-report-1" class="slide level2">
<h2>What to report?</h2>
<p>Procedure:</p>
<ul>
<li>Apparatus (eye-tracker, sampling rate, monitor, calibration procedure, distance to monitor/ET, which eye was recorded…)</li>
<li>Trial structure</li>
<li>Text layout (font, size, colours, number of lines, spacing…)</li>
<li>Position of the critical IAs on the screen</li>
<li>Task (how were the responses recorded?)</li>
</ul>
<p>Spoiler: analysis (data pre-processing, critical IAs, choice of measures).</p>
</section>
<section id="pros-cons" class="slide level2">
<h2>Pros &amp; cons</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Pros</strong></p>
<ul>
<li>Naturalistic reading.</li>
<li>Non-invasive method.</li>
<li>Different stages of processing.</li>
<li>Combination with offline measures possible.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Cons</strong></p>
<ul>
<li>Literacy required.</li>
<li>Many dependent variables.</li>
<li>Many potential confounds.</li>
<li>Resource-costly.</li>
</ul>
</div></div>
</section></section>
<section>
<section id="data-pre-processing" class="title-slide slide level1 center">
<h1>Data pre-processing</h1>

</section>
<section id="data-processing" class="slide level2">
<h2>Data processing</h2>
<p>Today:</p>
<ul>
<li><span class="fg" style="--col: #e64173">Pre-process data</span></li>
<li><span class="fg" style="--col: #e64173">Export data for analysis</span></li>
<li>Visualization</li>
<li>Data wrangling</li>
<li>Analysis</li>
</ul>
</section>
<section id="data-pre-processing-1" class="slide level2">
<h2>Data pre-processing</h2>
<ol type="1">
<li>Import data into software
<ul>
<li>Time window (= Interest period)</li>
</ul></li>
<li>Assess data
<ul>
<li>(Manual) checks</li>
</ul></li>
<li>(Automatized) cleaning</li>
<li>Prepare data for analysis &amp; export
<ul>
<li>Interest Area report</li>
</ul></li>
</ol>
</section></section>
<section>
<section id="reading-pipeline" class="title-slide slide level1 center">
<h1>Reading pipeline</h1>

</section>
<section id="interest-period" class="slide level2">
<h2>Interest period</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br></p>
<p>First make sure your IAs are displayed.</p>
<ul>
<li>Preferences &gt; Data Filters &gt; Show IAs</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/showIAs.png" class="quarto-figure quarto-figure-center" width="400"></p>
</figure>
</div>
</div></div>
</section>
<section id="interest-period-1" class="slide level2">
<h2>Interest period</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br></p>
<p>From stimulus presentation to button press (trial end)</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session3/interest_period.png" class="quarto-figure quarto-figure-center" width="500"></p>
</figure>
</div>
</div></div>
</section>
<section id="check-data" class="slide level2">
<h2>Check data</h2>
<p>Trial-by-trial check.</p>
<p>More time-consuming than VWP but more conventional.</p>
<ul>
<li>Drift (vertical, horizontal)</li>
<li>Blinks</li>
<li>Unusual behaviours</li>
</ul>
</section>
<section id="check-data-1" class="slide level2">
<h2>Check data</h2>
<p>Horizontal drift</p>
<ul>
<li>Safest -&gt; exclude such trials.</li>
<li>Especially when you are unsure which IAs certain fixations belong to.</li>
<li><strong>Never</strong> move fixations to the left or right.</li>
</ul>

<img data-src="_images/_session3/mariia_horizdrift.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="check-data-2" class="slide level2">
<h2>Check data</h2>
<p>Vertical drift</p>
<ul>
<li>Correct (move up/down) fixations that fall outside IAs
<ul>
<li>Less work if top and bottom margins were big enough</li>
</ul></li>
<li>To move certain fixations up/down: Windows Alt+<em>arrow</em>, Mac Option+<em>arrow</em></li>
</ul>

<img data-src="_images/_session3/mariia_verticaldrift.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="check-data-3" class="slide level2">
<h2>Check data</h2>
<p>Blinks in critical IA</p>
<ul>
<li>Exclude trial? Optional
<ul>
<li>Individual participants’ behaviour -&gt; exclude participant?</li>
<li>Happens randomly across conditions, and excluding those trials leads to overall large data loss -&gt; ignore?</li>
</ul></li>
</ul>

<img data-src="_images/_session3/mariia_blink_target.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="check-data-4" class="slide level2">
<h2>Check data</h2>
<p>Other</p>
<ul>
<li>Participants were instructed to look at the corner when finished reading but went back to re-read</li>
<li>Remove those extra fixations</li>
</ul>

<img data-src="_images/_session3/sticker.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="check-data-5" class="slide level2">
<h2>Check data</h2>
<p>Other</p>
<ul>
<li>Started reading from the middle (critical IA)</li>
<li>Exclude trial</li>
</ul>

<img data-src="_images/_session3/middle.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="check-data-6" class="slide level2">
<h2>Check data</h2>
<p>Keep a detailed <strong>log</strong> of all changes!</p>

<img data-src="_images/_session3/reading_cleaning_logbook.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="automatized-cleaning" class="slide level2">
<h2>Automatized cleaning</h2>
<p>What are we thinking of?</p>
<ul>
<li>Extremely short and long fixations (outliers)</li>
<li>Fixations outside of IAs</li>
</ul>
<p>Familiarity with previous literature is <strong>key</strong>.</p>
<p>Balanced loss across conditions.</p>
</section>
<section id="automatized-cleaning-1" class="slide level2">
<h2>Automatized cleaning</h2>
<p>Different thresholds -&gt; report <em>exactly</em> what you did.</p>
<ul>
<li>Short outliers
<ul>
<li>Merge short nearby fixations and remove those that couldn’t be merged.</li>
<li>Or just remove all fixations that are, e.g., &lt; 100 ms (Sereno &amp; Rayner, 2003).</li>
<li>Merging might inflate fixation measures, nor merging might inflate skipping if short critical IA.</li>
</ul></li>
<li>Long outliers
<ul>
<li>Remove all fixations that are, e.g., &gt; 800 ms.</li>
</ul></li>
<li>Delete fixations outside IAs</li>
</ul>
</section>
<section id="automatized-cleaning-2" class="slide level2">
<h2>Automatized cleaning</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><br> <br> <br></p>
<p>In-built 4-stage fixation cleaning procedure.</p>
<p>Right-click on the whole folder.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session3/4stage_a.png"></p>
</div></div>
</section>
<section id="automatized-cleaning-3" class="slide level2">
<h2>Automatized cleaning</h2>
<p>These are settings from D. Drieghe’s lab (non-standard)!</p>

<img data-src="_images/_session3/4stage_b.png" class="quarto-figure quarto-figure-center r-stretch" width="400"><p><sup>Stage 1: merge fix shorter than 80 ms and less than 0.5 degrees away from each other. Stage 2: merge remaining fix shorter than 40 ms and less than 1.25 degrees away from each other. Finally (Stage 4), remove all remaining fixations that are shorter than 80 ms or longer than 800 ms. Also, remove all fixations that fall outside the IAs.</sup></p>
</section>
<section id="exporting-data" class="slide level2">
<h2>Exporting data</h2>
<p>Recap: Fixations and saccades <strong>in several IAs</strong> (<em>n-1, n, n+1</em>)</p>
<ul>
<li>Interest Area report</li>
</ul>

<img data-src="_images/_session3/IAreport.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="exporting-data-1" class="slide level2">
<h2>Exporting data</h2>
<p>Interest Area report</p>
<ul>
<li>Choose variables (or just export all)</li>
</ul>

<img data-src="_images/_session3/variables_report.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="exporting-data-2" class="slide level2">
<h2>Exporting data</h2>
<p>Interest Area report</p>
<p>For analysis, we surely need participant ID (Session_Name_), condition, item ID, and interest area ID (IA_ID). It could also be helpful to include IA_LABEL (text presented in the IA).</p>
<p>Let’s choose some common measures:</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Skipping</li>
<li>First fixation duration</li>
<li>Gaze duration</li>
<li>Regressions-in</li>
<li>First-pass regressions-out</li>
<li>Regression path duration</li>
<li>Total reading time</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session3/var_choose.png"></p>
</div></div>
</section>
<section id="output" class="slide level2">
<h2>Output</h2>
<p>Interest Area report</p>
<p>.txt → import in Excel and save as .xlsx or .csv</p>

<img data-src="_images/_session3/IAreport_view.png" class="quarto-figure quarto-figure-center r-stretch"><ul>
<li>Here all IAs are listed; for analysis, we look at each IA separately!</li>
</ul>
</section>
<section id="regions-skipped-on-first-pass" class="slide level2">
<h2>Regions skipped on first pass</h2>
<p>After we exported our Interest Area report and before starting the analysis:</p>
<ul>
<li><p>In some trials, the region could have been skipped on first pass (IA_SKIP = 1).</p></li>
<li><p>This affects subsequent reading measures (first fixation? re-reading? etc.)</p></li>
<li><p>Therefore, we can report and analyze the skipping rate if we want;</p></li>
<li><p>For other measures, if IA_SKIP = 1, we will change the values to <em>NA</em> (label them as <em>missing values</em>) before we analyze them.</p></li>
<li><p>First-pass measures for sure: e.g., first fixation duration, gaze duration, first-pass regressions-out, regression-path duration.</p></li>
</ul>
</section>
<section id="regions-skipped-completely" class="slide level2">
<h2>Regions skipped completely</h2>
<p>If IA_SKIP = 1, but we don’t want to just assign <em>NA</em> to all measures indiscriminately:</p>
<ul>
<li><p>Later measures (e.g., regressions-in, total reading time): we can assign <em>NA</em> if the region was skipped completely (i.e., if IA_DWELL_TIME = 0).</p>
<ul>
<li><p>For regressions-in it’s already the default;</p></li>
<li><p>For total reading time the default is 0. Why would we want <em>NA</em> instead of 0? It might be better for statistical analysis (certain models don’t deal well with zero values).</p></li>
</ul></li>
</ul>
</section>
<section id="example" class="slide level2">
<h2>Example</h2>
<ul>
<li>What technical information do we expect to find?</li>
</ul>
</section>
<section id="example-1" class="slide level2">
<h2>Example</h2>
<ul>
<li>What technical information do we expect to find?</li>
</ul>
<p>Puebla &amp; Felser (2022)</p>

<img data-src="_images/_session3/puebla.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="what-to-report-2" class="slide level2">
<h2>What to report?</h2>
<p>Data preparation:</p>
<ul>
<li>Vertical drift correction (% corrected fixations)</li>
<li>Thresholds for removing short and long fixations, merging, fixations outside IAs (% data loss)</li>
<li>Other reasons for data exclusion, e.g., horizontal drift, track loss, removal of individual fixations (% data loss)</li>
<li>Skipping in first-pass? (% data loss)</li>
<li>Blinks? (% data loss)</li>
</ul>
</section>
<section id="what-to-report-3" class="slide level2">
<h2>What to report?</h2>
<p>Holmqvist et al.&nbsp;(2022). Eye tracking: Empirical foundations for a minimal reporting guideline.<sup>1</sup></p>
<p>Jakobi et al.&nbsp;(2024). Reporting eye-tracking data quality: Towards a new standard.</p>
<aside><ol class="aside-footnotes"><li id="fn6"><p>This paper was retracted due to some legal issues.</p></li></ol></aside></section>
<section id="take-home-messages" class="slide level2">
<h2>Take-home messages</h2>
<ul>
<li>Coding your experiment efficiently (e.g., meaningful variable names) saves you some time during pre-processing and data wrangling.</li>
<li>Eye-tracking data consists of time-stamped eye movement events and messages.</li>
<li>Software like Data Viewer make it easy to export what we are interested in.</li>
<li>Reading: important to decide on cleaning criteria beforehand and report them. Remember that pre-processing (trial-by-trial check) might take time.</li>
</ul>
</section>
<section id="practice-time" class="slide level2">
<h2>Practice time</h2>
<p>Have a go at some data from the training data sets! Afterwards we’ll compare our solutions.</p>
</section>
<section id="plan-for-tomorrow" class="slide level2">
<h2>Plan for tomorrow</h2>
<p>Visit to the lab!</p>
</section>
<section id="references" class="slide level2 smaller">
<h2>References</h2>
<p>Blythe, H. I., Häikiö, T., Bertam, R., Liversedge, S. P., &amp; Hyönä, J. (2011). Reading disappearing text: Why do children refixate words? <em>Vision Research, 51</em>(1), 84–92. https://doi.org/10.1016/j.visres.2010.10.003</p>
<p>Boland, J. E. (2004). Linking eye movements to sentence comprehension in reading and listening. <em>The on-line study of sentence comprehension: Eyetracking, ERP, and beyond</em>, 51–76.</p>
<p>Brysbaert, M. (2019). How many words do we read per minute? A review and meta-analysis of reading rate. <em>Journal of Memory and Language, 109</em>, 104047. https://doi.org/10.1016/j.jml.2019.104047</p>
<p>Carrol, G., Conklin, K., &amp; Gyllstad, H. (2016). FOUND IN TRANSLATION: The Influence of the L1 on the Reading of Idioms in a L2. <em>Studies in Second Language Acquisition, 38</em>(3), 403–443. https://doi.org/10.1017/S0272263115000492</p>
<p>Carroll, T. (2017). Eye Behavior While Reading Words of Sanskrit and Urdu Origin in Hindi. Brigham Young University.</p>
</section>
<section id="references-1" class="slide level2 smaller">
<h2>References</h2>
<p>Clifton, C., Staub, A., &amp; Rayner, K. (2007). Eye movements in reading words and sentences. <em>Eye movements</em>, 341–371.</p>
<p>Clifton, C., Ferreira, F., Henderson, J. M., Inhoff, A. W., Liversedge, S. P., Reichle, E. D., &amp; Schotter, E. R. (2016). Eye movements in reading and information processing: Keith Rayner’s 40year legacy. <em>Journal of Memory and Language, 86</em>, 1–19. https://doi.org/10.1016/j.jml.2015.07.004</p>
<p>Conklin, K., Pellicer-Sánchez, A., &amp; Carrol, G. (2018). Eye-Tracking: A Guide for Applied Linguistics Research. <em>Cambridge University Press</em>. https://doi.org/10.1017/9781108233279</p>
<p>Cop, U., Dirix, N., Drieghe, D., &amp; Duyck, W. (2017). Presenting GECO: An eyetracking corpus of monolingual and bilingual sentence reading. <em>Behavior Research Methods, 49</em>(2), 602–615. https://doi.org/10.3758/s13428-016-0734-0</p>
<p>Cornelis, X., Dirix, N., &amp; Bogaerts, L. (2025). The timing of spontaneous eye blinks in text reading suggests cognitive role. Scientific Reports, 15(1), 19849. https://doi.org/10.1038/s41598-025-04839-y</p>
</section>
<section id="references-2" class="slide level2 smaller">
<h2>References</h2>
<p>Dirix, N., Vander Beken, H., De Bruyne, E., Brysbaert, M., &amp; Duyck, W. (2019). Reading Text When Studying in a Second Language: An Eye-Tracking Study. <em>Reading Research Quarterly, 55</em>(3), 371–397. https://doi.org/10.1002/rrq.277</p>
<p>Drieghe, D. (2011). Parafoveal-on-foveal effects on eye movements during reading. In S. P. Liversedge, I. Gilchrist, &amp; S. Everling (Eds.), <em>The Oxford handbook of eye movements</em> (pp.&nbsp;840–855). Oxford: Oxford University Press.</p>
<p>Frazier, L., &amp; Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. <em>Cognitive Psychology, 14</em>(2), 178–210. https://doi.org/10.1016/0010-0285(82)90008-1</p>
<p>Frisson, S., Harvey, D. R., &amp; Staub, A. (2017). No prediction error cost in reading: Evidence from eye movements. <em>Journal of Memory and Language, 95</em>, 200–214. https://doi.org/10.1016/j.jml.2017.04.007</p>
<p>(RETRACTED) Holmqvist, K., Örbom, S. L., Hooge, I. T. C., Niehorster, D. C., Alexander, R. G., Andersson, R., Benjamins, J. S., Blignaut, P., Brouwer, A.-M., Chuang, L. L., Dalrymple, K. A., Drieghe, D., Dunn, M. J., Ettinger, U., Fiedler, S., Foulsham, T., van der Geest, J. N., Hansen, D. W., Hutton, S. B., … Hessels, R. S. (2023). Eye tracking: Empirical foundations for a minimal reporting guideline. Behavior Research Methods, 55(1), 364–416. https://doi.org/10.3758/s13428-021-01762-8</p>
</section>
<section id="references-3" class="slide level2 smaller">
<h2>References</h2>
<p>Howard, P. L., Liversedge, S. P., &amp; Benson, V. (2017). Processing of co-reference in autism spectrum disorder. <em>Autism Research, 10</em>(12), 1968–1980. https://doi.org/10.1002/aur.1845</p>
<p>Hyönä, J. (2011). Foveal and parafoveal processing during reading. In S. P. Liversedge, I. Gilchrist, &amp; S. Everling (Eds.), <em>The Oxford handbook of eye movements</em> (pp.&nbsp;820–838). Oxford: Oxford University Press.</p>
<p>Hyönä, J., Olson, R., Defries, J., Fulker, D., Pennington, B., &amp; Smith, S. (1995). Eye Fixation Patterns Among Dyslexic and Normal Readers: Effects of Word Length and Word Frequency. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition, 21</em>, 1430–1440. https://doi.org/10.1037/0278-7393.21.6.1430</p>
<p>Jakobi, D. N., Krakowczyk, D. G., &amp; Jäger, L. A. (2024). Reporting Eye-Tracking Data Quality: Towards a New Standard. <em>ETRA ’24: Proceedings of the 2024 Symposium on Eye Tracking Research and Applications.</em> https://doi.org/10.1145/3649902.3655658</p>
<p>Jayes, L. T., Fitzsimmons, G., Weal, M. J., Kaakinen, J. K., &amp; Drieghe, D. (2022). The impact of hyperlinks, skim reading and perceived importance when reading on the Web. PLOS ONE, 17(2), e0263669. https://doi.org/10.1371/journal.pone.0263669</p>
</section>
<section id="references-4" class="slide level2 smaller">
<h2>References</h2>
<p>Juhasz, B. J., Pollatsek, A., Hyönä, J., Drieghe, D., &amp; Rayner, K. (2009). Parafoveal processing within and between words. <em>Quarterly Journal of Experimental Psychology, 62</em>(7), 1356–1376. https://doi.org/10.1080/17470210802400010</p>
<p>Juhasz, B. J., &amp; Sheridan, H. (2020). The time course of age-of-acquisition effects on eye movements during reading: Evidence from survival analyses. Memory &amp; Cognition, 48(1), 83–95. https://doi.org/10.3758/s13421-019-00963-z</p>
<p>Just, M. A., &amp; Carpenter, P. A. (1980). A theory of reading: from eye fixations to comprehension. <em>Psychological review, 87</em>(4), 329.</p>
<p>Keating, G. D. (2009). Sensitivity to Violations of Gender Agreement in Native and Nonnative Spanish: An Eye-Movement Investigation. <em>Language Learning, 59</em>(3), 503–535. https://doi.org/10.1111/j.1467-9922.2009.00516.x</p>
<p>Liversedge, S. P., Rayner, K., White, S. J., Vergilino-Perez, D., Findlay, J. M., &amp; Kentridge, R. W. (2004). Eye movements when reading disappearing text: is there a gap effect in reading?. <em>Vision research, 44</em>(10), 1013–1024.</p>
</section>
<section id="references-5" class="slide level2 smaller">
<h2>References</h2>
<p>McConkie, G. W., &amp; Rayner, K. (1975). The span of the effective stimulus during a fixation in reading. <em>Perception &amp; Psychophysics, 17</em>, 578–586.</p>
<p>Mitchell, D. C., Shen, X., Green, M. J., &amp; Hodgson, T. L. (2008). Accounting for regressive eye-movements in models of sentence processing: A reappraisal of the Selective Reanalysis hypothesis. <em>Journal of Memory and Language, 59</em>(3), 266–293.</p>
<p>Mlinarič, M., Los, S. A., &amp; Snell, J. (2025). On the spatial limits of parallel word processing in reading. Attention, Perception &amp; Psychophysics. https://doi.org/10.3758/s13414-025-03101-x</p>
<p>Morrison, R. E. (1984). Manipulation of stimulus onset delay in reading: Evidence for parallel programming of saccades. <em>Journal of Experimental Psychology: Human Perception and Performance, 10</em>(5), 667–682. https://doi.org/10.1037/0096-1523.10.5.667</p>
<p>Pellicer-Sánchez, A. (2016). INCIDENTAL L2 VOCABULARY ACQUISITION FROM AND WHILE READING: An Eye-Tracking Study. <em>Studies in Second Language Acquisition, 38</em>(1), 97–130. https://doi.org/10.1017/S0272263115000224</p>
</section>
<section id="references-6" class="slide level2 smaller">
<h2>References</h2>
<p>Pickering, M. J., Frisson, S., McElree, B., &amp; Traxler, M. J. (2004). Eye Movements and Semantic Composition. In M. Carreiras &amp; C. Clifton Jr.&nbsp;(Eds.), <em>The On-line Study of Sentence Comprehension</em>. Psychology Press.</p>
<p>Puebla, C., &amp; Felser, C. (2022). Discourse Prominence and Antecedent Mis-Retrieval during Native and Non-Native Pronoun Resolution. Discours. Revue de Linguistique, Psycholinguistique et Informatique. <em>A Journal of Linguistics, Psycholinguistics and Computational Linguistics, 29</em>, Article 29. https://doi.org/10.4000/discours.11720</p>
<p>Rayner, K. (1975). The perceptual span and peripheral cues in reading. <em>Cognitive Psychology, 7</em>, 65–81.</p>
<p>Rayner, K., &amp; Bertera, J. H. (1979). Reading without a fovea. <em>Science, 206</em>, 468–469.</p>
<p>Rayner, K., &amp; Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. <em>Memory &amp; Cognition, 14</em>(3), 191–201. https://doi.org/10.3758/BF03197692</p>
</section>
<section id="references-7" class="slide level2 smaller">
<h2>References</h2>
<p>Sereno, S. C., &amp; Rayner, K. (1992). Fast priming during eye fixations in reading. <em>Journal of Experimental Psychology: Human Perception and Performance, 18</em>(1), 173.</p>
<p>Sereno, S. C., &amp; Rayner, K. (2003). Measuring word recognition in reading: Eye movements and event-related potentials. <em>Trends in Cognitive Sciences, 7</em>(11), 489–493. https://doi.org/10.1016/j.tics.2003.09.010</p>
<p>Solan, H. A., Feldman, J., &amp; Tujak, L. (1995). Developing Visual and Reading Efficiency in Older Adults. <em>Optometry and Vision Science, 72</em>(2), 139.</p>
<p>Staub, A., Rayner, K., Pollatsek, A., Hyönä, J., &amp; Majewski, H. (2007). The time course of plausibility effects on eye movements in reading: Evidence from noun-noun compounds. <em>Journal of Experimental Psychology. Learning, Memory, and Cognition, 33</em>(6), 1162–1169. https://doi.org/10.1037/0278-7393.33.6.1162</p>
<p>von der Malsburg, T., &amp; Angele, B. (2017). False positives and other statistical errors in standard analyses of eye movements in reading. Journal of Memory and Language, 94, 119–133. https://doi.org/10.1016/j.jml.2016.10.003</p>
<div class="quarto-auto-generated-content">
<p><img src="_images/logo_mils.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>Reading</p>
</div>
</div>
</section></section>

    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="session3_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="session3_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="session3_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="session3_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="session3_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="session3_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="session3_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="session3_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="session3_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="session3_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    <script>videojs(video_shortcode_videojs_video2);</script>
    <script>videojs(video_shortcode_videojs_video3);</script>
    <script>videojs(video_shortcode_videojs_video4);</script>
    <script>videojs(video_shortcode_videojs_video5);</script>
    

</body></html>