<!DOCTYPE html>
<html lang="en"><head>
<script src="session_1_2025_files/libs/clipboard/clipboard.min.js"></script>
<script src="session_1_2025_files/libs/quarto-html/tabby.min.js"></script>
<script src="session_1_2025_files/libs/quarto-html/popper.min.js"></script>
<script src="session_1_2025_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="session_1_2025_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session_1_2025_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="session_1_2025_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="session_1_2025_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="session_1_2025_files/libs/quarto-contrib/videojs/video.min.js"></script>
<link href="session_1_2025_files/libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.538">

  <meta name="author" content="Esperanza Badaya">
  <title>Introduction to Eye-tracking I</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="session_1_2025_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="session_1_2025_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="session_1_2025_files/libs/revealjs/dist/theme/quarto.css">
  <link href="session_1_2025_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="session_1_2025_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="session_1_2025_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="session_1_2025_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <style>
  .center-xy {
    margin: 0;
    position: absolute;
    top: 40%;
    left: 40%;
    -ms-transform: translateY(-50%), translateX(-50%);
    transform: translateY(-50%), translateX(-50%);
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Introduction to Eye-tracking I</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Esperanza Badaya 
</div>
</div>
</div>

</section>
<section>
<section id="eye-tracking-in-language-research" class="title-slide slide level1 center">
<h1>Eye-tracking in Language Research</h1>

</section>
<section id="welcome" class="slide level2">
<h2>Welcome</h2>
<p>The team:</p>
<ul>
<li>Email: esperanza.badaya@ugent.be</li>
<li>Working hours: Monday to Friday 9 to 17.
<ul>
<li>Response time: 24/48h.</li>
<li>Office hours: <em>Fridays</em> 10 to 12, office 140.005.</li>
</ul></li>
</ul>
</section>
<section id="welcome-1" class="slide level2">
<h2>Welcome</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Binger Lu</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/binger.png" height="300"></p>
<figcaption>Speech comprehension</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>Mariia Baltais</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/mariia.jpg" height="300"></p>
<figcaption>Reading</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="welcome-2" class="slide level2">
<h2>Welcome</h2>
<ul>
<li><p>What is your background?</p></li>
<li><p>What are your interests?</p></li>
<li><p>Do you have any experience in research?</p>
<ul>
<li>And with eye-tracking?</li>
</ul></li>
<li><p>What are your goals with this course? What would you like to get from it?</p></li>
</ul>
</section>
<section id="the-course" class="slide level2">
<h2>The course</h2>
<p>Goal: Introduction to the eye-tracking technique in the whole cycle of a research project.</p>
<ul>
<li>Focus on paradigms in psycholinguistics.</li>
</ul>
<p>In more practical terms, this means…</p>
</section>
<section id="overview-of-the-course-eye-tracking" class="slide level2">
<h2>Overview of the course: Eye-tracking</h2>
<ul>
<li>Introduction to eye-tracking
<ul>
<li>Why do we do it?</li>
<li>Measures</li>
</ul></li>
<li>Introduction to the technicalities of eye-tracking
<ul>
<li>How it works</li>
<li>How to conduct an experiment in the lab</li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-language-research" class="slide level2">
<h2>Overview of the course: Language research</h2>
<p>Language research:</p>
<ul>
<li>What do you want to study?
<ul>
<li>Speech comprehension?</li>
<li>Reading?</li>
<li>Speech production?</li>
<li>Writing?</li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-language-research-1" class="slide level2">
<h2>Overview of the course: Language research</h2>
<p>Language research:</p>
<ul>
<li>What do you want to study?
<ul>
<li>Speech comprehension? <strong>Visual World Paradigm</strong></li>
<li>Reading? <strong>Single sentence reading</strong></li>
<li>Speech production? <strong>Visual World Paradigm</strong></li>
<li>Writing?</li>
</ul></li>
</ul>
<p>Note: The focus won’t be the findings themselves (this is not a psycholinguistics course).</p>
</section>
<section id="overview-of-the-course-cycle-of-a-research-project" class="slide level2">
<h2>Overview of the course: Cycle of a research project</h2>
<ul>
<li>Design
<ul>
<li>What each paradigm is</li>
<li>Elements of the paradigm</li>
<li>Eye-tracking component</li>
<li>Confounding factors</li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-cycle-of-a-research-project-1" class="slide level2">
<h2>Overview of the course: Cycle of a research project</h2>
<ul>
<li>Programming and conducting the experiment
<ul>
<li>Software</li>
<li>How to run the experiment
<ul>
<li>Troubleshooting</li>
<li>Data acquisition</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-cycle-of-a-research-project-2" class="slide level2">
<h2>Overview of the course: Cycle of a research project</h2>
<ul>
<li>Data pre-processing
<ul>
<li>The nature of our data</li>
<li>Data exploration</li>
<li>Participant and trial data</li>
<li>Lab log</li>
<li>Data cleaning</li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-cycle-of-a-research-project-3" class="slide level2">
<h2>Overview of the course: Cycle of a research project</h2>
<ul>
<li>Analysis
<ul>
<li>The nature of our data</li>
<li>Data visualization</li>
<li>Data transformations</li>
<li>Statistical analysis</li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-cycle-of-a-research-project-4" class="slide level2">
<h2>Overview of the course: Cycle of a research project</h2>
<ul>
<li>Reporting
<ul>
<li>Reproductibility</li>
<li>Transparency</li>
<li>Eye-tracking details</li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-organisation" class="slide level2">
<h2>Overview of the course: Organisation</h2>
<p>4 preliminary “theoretical classes”</p>
<p>6 practical classes</p>
<ul>
<li>Coding session</li>
<li>Lab visits &amp; data collection</li>
<li>Pre-processing session</li>
<li>Data analysis session</li>
</ul>
<p>1 “wild card” class: Preparation for the report &amp; additional topics</p>
</section>
<section id="overview-of-the-course" class="slide level2">
<h2>Overview of the course</h2>
<p>If we put all of these together we have:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">12th Feb</td>
<td style="text-align: center;">Introduction to eye-tracking</td>
</tr>
<tr class="even">
<td style="text-align: center;">19th Feb</td>
<td style="text-align: center;">Introduction to eye-tracking</td>
</tr>
<tr class="odd">
<td style="text-align: center;">26th Feb</td>
<td style="text-align: center;">Visual World Paradigm</td>
</tr>
<tr class="even">
<td style="text-align: center;">5th Mar</td>
<td style="text-align: center;">Reading</td>
</tr>
<tr class="odd">
<td style="text-align: center;">12th Mar</td>
<td style="text-align: center;">1st coding session</td>
</tr>
<tr class="even">
<td style="text-align: center;">19th Mar</td>
<td style="text-align: center;">2nd coding session + Exam</td>
</tr>
<tr class="odd">
<td style="text-align: center;">26th Mar</td>
<td style="text-align: center;">1st Lab session (in groups)</td>
</tr>
<tr class="even">
<td style="text-align: center;">2nd Apr</td>
<td style="text-align: center;">2nd Lab session (testing)</td>
</tr>
</tbody>
</table>
</section>
<section id="overview-of-the-course-1" class="slide level2">
<h2>Overview of the course</h2>
<p>If we put all of these together we have:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Date</th>
<th style="text-align: center;">Content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">23rd Apr</td>
<td style="text-align: center;">Data pre-processing</td>
</tr>
<tr class="even">
<td style="text-align: center;">30th Apr</td>
<td style="text-align: center;">Data analysis and reporting</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7th May</td>
<td style="text-align: center;">Mini lecture</td>
</tr>
<tr class="even">
<td style="text-align: center;">14th May</td>
<td style="text-align: center;">Presentation</td>
</tr>
</tbody>
</table>
<p><strong>NB</strong>: Presentation date may be moved to the 21st</p>
</section>
<section id="overview-of-the-course-organisation-1" class="slide level2">
<h2>Overview of the course: Organisation</h2>
<p>Slides + readings</p>
<ul>
<li>Articles</li>
<li>Ufora “forum”</li>
<li>Experiment Builder tutorials
<ul>
<li>Templates of scripts &amp; learn about its components</li>
</ul></li>
</ul>
</section>
<section id="technical-aspect-of-the-course" class="slide level2">
<h2>Technical aspect of the course</h2>
<p>Experiment Builder.</p>
<ul>
<li>This course is designed assuming zero knowledge of coding.</li>
</ul>
<p><strong>Problem</strong>: Experiment Builder is licensed.</p>
<p><strong>Solution</strong> (?): Trial version on Ufora -&gt; build skills -&gt; coding session with dongle.</p>
</section>
<section id="technical-aspect-of-the-course-1" class="slide level2">
<h2>Technical aspect of the course</h2>
<p>Experiment Builder tutorials</p>
<ul>
<li>Exercises are not mandatory</li>
<li>Some terms will become clearer the more we learn about eye-tracking.</li>
<li>There are also Q&amp;A sections without deadlines.</li>
<li>You can progress at the speed you want (but I will state by when a certain module must have been read).</li>
</ul>
</section>
<section id="technical-aspect-of-the-course-2" class="slide level2">
<h2>Technical aspect of the course</h2>
<p>If you are comfortable with Python, and want to code the experiment in PsychoPy/OpenSesame, you are welcome to do so.</p>
<ul>
<li>This won’t grand you a higher grade (for fairness’s sake)</li>
<li>Some designs (e.g., for speech comprehension) are easier than others (e.g., reading)</li>
<li>Come talk to me if you are considering this venue.</li>
</ul>
</section>
<section id="overview-of-the-course-evaluation" class="slide level2">
<h2>Overview of the course: Evaluation</h2>
<p>2 components: Exam and project</p>
<ul>
<li>Exam: 20%</li>
<li>Project: 80%
<ul>
<li>20%: Presentation</li>
<li>60%: Written report</li>
</ul></li>
</ul>
</section>
<section id="overview-of-the-course-project" class="slide level2">
<h2>Overview of the course: Project</h2>
<p>7 teams (4 members)</p>
<ul>
<li>Team up with people with similar interest and/or complementary skills.</li>
<li>Deadline: Next Tuesday</li>
<li>Topic: An original idea or a replication of a paper.</li>
</ul>
</section>
<section id="overview-of-the-course-project-1" class="slide level2 smaller">
<h2>Overview of the course: Project</h2>
<p>~ Registered report</p>
<ul>
<li>Introduction: Rationale for the experiment and whether eye-tracking is appropriate.</li>
<li>Hypotheses: Map the research question onto eye-tracking measures.</li>
<li>Methods: Materials, participant sample, considering confounding factors.</li>
<li>Procedure: Experiment procedure + eye-tracking components</li>
<li>Data pre-processing: Description of data cleaning, transformations.</li>
<li>Data analysis and results: Descriptions of model used (and whether it answers the question) + visualizations</li>
<li>Discussion</li>
</ul>
<p>All in APA-style, clearly written and structured.</p>
<ul>
<li>Transparency and reproducibility.</li>
</ul>
</section>
<section id="overview-of-the-course-project-2" class="slide level2">
<h2>Overview of the course: Project</h2>
<p>~Conference presentation</p>
<ul>
<li>Introduction, research question &amp; hypotheses, method, results</li>
<li>Data visualization</li>
<li>Clarity of exposition</li>
<li>Engagement with post-presentation questions</li>
</ul>
</section>
<section id="overview-of-the-course-deadlines" class="slide level2">
<h2>Overview of the course: Deadlines</h2>
<ul>
<li>18th Feb: Group nomination</li>
<li>11th Mar: Discussion of research project</li>
<li>19th Mar: Exam</li>
<li>6th Apr: Submission write-up project (non-assessed)</li>
<li>13th May: Submission presentations</li>
<li>8th June: Written report</li>
</ul>
</section>
<section class="slide level2">

<div class="center-xy">
<p>Questions thus far?</p>
</div>
</section></section>
<section>
<section id="introduction-to-eye-tracking-i" class="title-slide slide level1 center">
<h1>Introduction to eye-tracking I</h1>

</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<ol type="1">
<li>What is eye-tracking?</li>
<li>Why do we track eyes?
<ol type="i">
<li>The human visual system</li>
<li>Visual attention</li>
</ol></li>
<li>Eye movements</li>
<li>Brief introduction to language research</li>
</ol>
</section>
<section id="what-is-eye-tracking" class="slide level2">
<h2>What is eye-tracking?</h2>
<p><a href="https://www.youtube.com/watch?v=TDG6j2LkVqU">Flying object</a></p>
<aside class="notes">
<p>What do you think is eye-tracking? As obvious as the question is.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-is-eye-tracking-1" class="slide level2">
<h2>What is eye-tracking?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Eye-tracking is a <span class="fg" style="--col: #e64173">non-invasive technique</span> to explore cognitive processes as they unfold (i.e., <span class="fg" style="--col: #e64173">online processing</span>) via eye movements. It has temporal resolution.</p>
<ul>
<li>Opposed to EEG, fMRI</li>
<li>Opposed to offline measures</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session1/et.JPG"></p>
</div>
</div>
</section></section>
<section>
<section id="why-do-we-track-eyes" class="title-slide slide level1 center">
<h1>Why do we track eyes?</h1>

</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system" class="slide level2">
<h2>Why do we move our eyes? The Human Visual System</h2>

<img data-src="_images/_session1/fovea_parafovea.JPG" class="quarto-figure quarto-figure-center r-stretch"><p class="caption">Younis, Nuiamy, &amp; Alomari (2019), note that foveal is not central vision</p><aside class="notes">
<p>Each eye covers about 150° horizontally, but the overlap creates a combined binocular field of about 120°–140°. Staring ahead</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system-1" class="slide level2">
<h2>Why do we move our eyes? The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Important parts of the eye’s anatomy:</p>
<ul>
<li>Cornea</li>
<li>Pupil</li>
<li>Retina
<ul>
<li>Fovea</li>
<li>Parafovea</li>
<li>Periphery</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session1/eye_anatomy.png"></p>
</div>
</div>
</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system-2" class="slide level2">
<h2>Why do we move our eyes? The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Light hits the cornea.
<ul>
<li>Some light is reflected (Purkinje reflections)</li>
</ul></li>
<li>Light enters the eye via the pupil.</li>
<li>The lens focuses light onto the retina.
<ul>
<li>Size of the image is measured in visual angles.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session1/eye_anatomy.png"></p>
</div>
</div>
</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system-3" class="slide level2 smaller">
<h2>Why do we move our eyes? The Human Visual System</h2>
<p>What are visual angles?</p>
<ul>
<li>Interplay between object size and distance.
<ul>
<li>Bigger objects = larger retinal images.</li>
<li>Closer objects = larger retinal images.</li>
</ul></li>
<li>Larger visual angles = objects appear bigger in your vision.
<ul>
<li>How much of the visual field an object occupies.
<ul>
<li>Spoiler: Consequences for lab set up and stimuli presentation.</li>
</ul></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/visual_angle.JPG" class="quarto-figure quarto-figure-center" height="200"></p>
</figure>
</div>
</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system-4" class="slide level2">
<h2>Why do we move our eyes? The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The lens reflects the light onto the retina.
<ul>
<li>Photosensitive layer with <span class="fg" style="--col: #e64173">cones</span> and <span class="fg" style="--col: #e64173">rodes</span>.</li>
<li>Light is transformed into electrical signals.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session1/eye_anatomy.png"></p>
</div>
</div>
</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system-5" class="slide level2">
<h2>Why do we move our eyes? The Human Visual System</h2>
<p>Photoreceptors with different properties (e.g., spectral sensitivity, photopigments).</p>
<ul>
<li>Cones
<ul>
<li><span class="fg" style="--col: #e64173">Color vision and spatial frequency</span> (“visual details”).</li>
<li>Well-illuminated conditions (photopic vision).</li>
</ul></li>
<li>Rods
<ul>
<li><span class="fg" style="--col: #e64173">Sensitive to light</span>.</li>
<li>Low-light conditions (scotopic vision).</li>
</ul></li>
</ul>
</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system-6" class="slide level2">
<h2>Why do we move our eyes? The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>They also differ in where they are located in the retina.</p>
<ul>
<li>Cones: Highest density in the <span class="fg" style="--col: #e64173">fovea</span>.</li>
<li>Rodes: Density increases as we <span class="fg" style="--col: #e64173">move away from the fovea</span>.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session1/rod_distribution.png"></p>
</div>
</div>
</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system-7" class="slide level2">
<h2>Why do we move our eyes? The Human Visual System</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Retina:</p>
<ul>
<li>Fovea: 1-2 degrees of visual angle.</li>
<li>Parafovea: 10 degrees of visual angle to either side.</li>
<li>Peripheria: Remaining space beyond the parafovea.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-layout-panel" data-layout="[[-1], [1], [-1]]">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/example_fovea.JPG" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="why-do-we-move-our-eyes-the-human-visual-system-8" class="slide level2">
<h2>Why do we move our eyes? The Human Visual System</h2>
<p><br></p>
<div style="text-align: center">
<p>Therefore, we move our eyes to place visual stimuli in the fovea to process it with the highest acuity. <span class="fg" style="--col: #e64173">Eye movements are a consequence of the eyes’ anatomy</span>.</p>
</div>
<ul>
<li>Parafoveal processing: No acute image, words still partially recognizable.</li>
<li>Peripheria: Blurred image, no word/letter recognition.</li>
</ul>
</section>
<section id="why-do-we-do-it-visual-attention" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<p>What do you notice about the eye movements here? What do you infer from them?</p>
<p><a href="https://www.youtube.com/watch?v=jzeBKRjWVwE">Playing a video game</a></p>
</section>
<section id="why-do-we-do-it-visual-attention-1" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<p><span class="fg" style="--col: #e64173">Attention</span> (i.e., <span class="fg" style="--col: #e64173">linking hypothesis</span>)</p>
<ul>
<li>Tracking eye movements can tell us what viewers are paying attention to.</li>
</ul>
</section>
<section id="why-do-we-do-it-visual-attention-2" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<p>Attention determines what we process and the detail of the representation built.</p>
<p><a href="https://www.youtube.com/watch?v=U1saQoMRD8A">Attention test</a></p>
</section>
<section id="why-do-we-do-it-visual-attention-3" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="fg" style="--col: #e64173">Attention</span> (i.e., <span class="fg" style="--col: #e64173">linking hypothesis</span>)</p>
<ul>
<li><span class="fg" style="--col: #e64173">Bottom-up</span> and <span class="fg" style="--col: #e64173">top-down</span> processes.
<ul>
<li>Details that attract individuals’ attention (exogeneous) v. Individuals’ strategies (endogeneous).</li>
</ul></li>
<li>Individuals as active viewers.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/endogenous.jpg"></p>
<figcaption>Open University</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="why-do-we-do-it-visual-attention-4" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<p>Yarbus (1967): Scanpaths guided by attention.</p>

<img data-src="_images/_session1/unexpected_visitor.png" class="quarto-figure quarto-figure-center r-stretch"><p class="caption">They Did Not Expect Him, Iliá Repin</p></section>
<section id="why-do-we-do-it-visual-attention-5" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<div class="quarto-layout-panel" data-layout="[15,-2,10]">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: center;">
<p><img data-src="_images/_session1/unexpected_visitor.png"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 7.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/yarbus_freeviewing.png"></p>
<figcaption>Free viewing</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="why-do-we-do-it-visual-attention-6" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<div class="quarto-layout-panel" data-layout="[15,-2,10]">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: center;">
<p><img data-src="_images/_session1/unexpected_visitor.png"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 7.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/yarbus_ages.png"></p>
<figcaption>Estimate individuals’ ages</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="why-do-we-do-it-visual-attention-7" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<div class="quarto-layout-panel" data-layout="[15,-2,10]">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 55.6%;justify-content: center;">
<p><img data-src="_images/_session1/unexpected_visitor.png"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 7.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/yarbus_guessactivity.png"></p>
<figcaption>Estimate what they were doing when the visitor arrived</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="why-do-we-do-it-visual-attention-8" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<p>Attention is a bridge between our minds and eye movements.</p>
<p><span class="fg" style="--col: #e64173">Potential caveat</span></p>
<ul>
<li>Covert versus overt attention:
<ul>
<li>Covert: Mental shift without physical evidence (e.g., looking at the slides while thinking about lunch).</li>
<li>Overt: Moving your eyes to check what time it is.</li>
</ul></li>
</ul>
<p>We only have access to overt attention.</p>
</section>
<section id="why-do-we-do-it-visual-attention-9" class="slide level2">
<h2>Why do we do it? Visual attention</h2>
<ul>
<li>In language<sup>1</sup>: The <span class="fg" style="--col: #e64173">eye-mind hypothesis</span> (Just &amp; Carpenter, 1980).
<ul>
<li><span class="fg" style="--col: #e64173">Where</span> we look indicates <span class="fg" style="--col: #e64173">what we are processing</span>, <span class="fg" style="--col: #e64173">for how long we look</span> indicates the <span class="fg" style="--col: #e64173">cognitive effort it takes to process it</span>.</li>
<li>This is an <strong>assumption</strong> (cf.&nbsp;Pickering et al., 2004).</li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn1"><p>NB later in the course this will be challenged, especially wrt speech comprehension.</p></li></ol></aside></section>
<section id="why-do-we-do-it-tldr" class="slide level2">
<h2>Why do we do it? TL;DR</h2>
<ul>
<li>Visual acuity is highest in the fovea, but the fovea is a rather small section of the retina.</li>
<li>Moving our eyes helps us to ‘place’ objects on the fovea.</li>
<li>Why do we want to place something there? Arguably, because we are interested in it.</li>
<li>We move our eyes to what captures our attention to process it.</li>
</ul>
</section></section>
<section>
<section id="eye-movements" class="title-slide slide level1 center">
<h1>Eye movements</h1>

</section>
<section id="eye-movements-binocular-nature" class="slide level2">
<h2>Eye movements: Binocular Nature</h2>
<ul>
<li>One dominant eye<sup>1</sup>.</li>
<li>Binocular disparity:
<ul>
<li>Relatively small in healthy subjects.</li>
<li>Decreases over the time of a fixation.</li>
<li>No complete temporal synchrony in eye movements.</li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn2"><p>More on this when we cover properly the lab set up.</p></li></ol></aside></section>
<section id="eye-movements-measures" class="slide level2">
<h2>Eye movements: Measures</h2>
<div style="text-align: center">
<p><strong>What eye movements can you think of?</strong></p>
<div>
<ul>
<li class="fragment">Think of how we talk about things: we <span class="fg" style="--col: #e64173">fixate</span> on things, we <span class="fg" style="--col: #e64173">move</span> our eyes.</li>
<li class="fragment">We also <span class="fg" style="--col: #e64173">blink</span>.</li>
<li class="fragment">Our <span class="fg" style="--col: #e64173">pupils</span> also change in size.</li>
</ul>
</div>
</div>
</section>
<section id="eye-movements-measures-1" class="slide level2">
<h2>Eye movements: Measures</h2>
<p><strong>What eye movements can you think of?</strong></p>
<div>
<ul>
<li class="fragment">Fixations</li>
<li class="fragment">Saccades</li>
<li class="fragment">Blinks</li>
<li class="fragment">Smooth pursuit</li>
<li class="fragment">Pupil size changes</li>
</ul>
</div>
</section>
<section id="eye-movements-measures-2" class="slide level2">
<h2>Eye movements: Measures</h2>
<p><strong>What eye movements can you think of?</strong></p>
<ul>
<li><strong>Fixations</strong></li>
<li><strong>Saccades</strong></li>
<li>Blinks</li>
<li>Smooth pursuit</li>
<li>Pupil size changes</li>
</ul>
</section>
<section id="eye-movements-fixations" class="slide level2">
<h2>Eye movements: Fixations</h2>
<p>When our eye ‘stops’ i.e., multiple gaze points close in time and/or space.</p>
<ul>
<li><p>Eye is <em>relatively</em> stable.</p></li>
<li><p>Average duration: 200 - 300 ms.</p></li>
<li><p>Minimal duration: 20 - 50 ms (not standard).</p></li>
<li><p>Time it takes to launch a saccade: ~ 200 ms (Matin et al., 1993).</p></li>
</ul>
</section>
<section id="eye-movements-fixations-1" class="slide level2">
<h2>Eye movements: Fixations</h2>
<p>When our eye ‘stops’.</p>
<ul>
<li>Eye is <em>relatively</em> stable.
<ul>
<li>Tremor, drifts, microsaccades.</li>
</ul></li>
</ul>

<img data-src="_images/_session1/fixation_tremor.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="eye-movements-fixations-2" class="slide level2">
<h2>Eye movements: Fixations</h2>

<img data-src="_images/_session1/micro_saccades.PNG" class="r-stretch quarto-figure-center"><p class="caption">Alexander &amp; Martinez-Conde (2019)</p></section>
<section id="eye-movements-fixations-3" class="slide level2">
<h2>Eye movements: Fixations</h2>
<ul>
<li>Tremor: Smallest of the movements.</li>
<li>Drifts: Slow movement away from the center of the fixation, happens between microsaccades.</li>
<li>Microsaccades: Move back the eye to the center of the fixation.</li>
</ul>
</section>
<section id="eye-movements-fixations-4" class="slide level2">
<h2>Eye movements: Fixations</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/troxler.jpg" class="quarto-figure quarto-figure-center" height="300"></p>
</figure>
</div>
</section>
<section id="eye-movements-fixations-5" class="slide level2">
<h2>Eye movements: Fixations</h2>
<ul>
<li>Tremor: Smallest of the movements.</li>
<li>Drifts: Slow movement away from the center of the fixation, happens between microsaccades.</li>
<li>Microsaccades: Move back the eye to the center of the fixation.
<ul>
<li>Perceptual fading: Troxler fading.</li>
<li>Only microsaccades can restore it, while drift and microsaccades prevent fading.</li>
</ul></li>
</ul>
<aside class="notes">
<p>On a separate but related note, in class, I said that only microsaccades can restore perceptual fading, while drifts and microsaccades prevent it, but I didn’t know why – I’ve done a quick read and here’s a potential answer: “microsaccades create more retinal motion than drifts or tremor, and thus they may be more efficient at driving certain perceptual and physiological effects than the other fixational eye movements types […] This is likely due to microsaccades’ higher speeds and the larger distances they cover compared to drift. Consistent with this proposal, larger microsaccades reverse fading more effectively than smaller ones do (McCamy, Macknik, et al., 2014; McCamy et al.,2012).” (Alexander &amp; Martínez-Conde, 2019).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-movements-saccades" class="slide level2">
<h2>Eye movements: Saccades</h2>

<img data-src="_images/_session1/fixation_saccades.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="eye-movements-saccades-1" class="slide level2">
<h2>Eye movements: Saccades</h2>
<p>“Jerky” movement: Fast movement of the eye, usually from one fixation to another.</p>
<ul>
<li>Temporary blindness (i.e., <span class="fg" style="--col: #e64173">saccadic suppression</span>).</li>
<li>Ballistic</li>
<li>Reactive saccades versus Voluntary saccades.
<ul>
<li>Sudden appearance of an object versus Exploration.</li>
</ul></li>
</ul>
</section>
<section id="eye-movements-saccades-2" class="slide level2">
<h2>Eye movements: Saccades</h2>
<p><a href="https://www.cse.iitk.ac.in/users/se367/10/presentation_local/Change%20Blindness.html">Change detection</a></p>
</section>
<section id="eye-movements-saccades-3" class="slide level2">
<h2>Eye movements: Saccades</h2>
<video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="_images/_session1/boundary.mp4"></video>
</section>
<section id="eye-movements-saccades-4" class="slide level2">
<h2>Eye movements: Saccades</h2>
<p>Parameters</p>
<ul>
<li>Amplitude: distance travelled.
<ul>
<li>Average: 15°.</li>
</ul></li>
<li>Temporal parameters (onset, offset, duration).
<ul>
<li>Average duration: 30 - 80 ms.</li>
</ul></li>
<li>Direction: Forward and backwards (i.e., regressions) saccades.</li>
<li>Over- and under-shooting &amp; corrections (glissades)</li>
</ul>
</section>
<section id="eye-movements-saccades-5" class="slide level2">
<h2>Eye movements: Saccades</h2>
<p>Forward and backwards (i.e., regressions) saccades.</p>
<ul>
<li>Short and long regressions.</li>
</ul>

<img data-src="_images/_session1/example_reading.JPG" class="quarto-figure quarto-figure-center r-stretch"><p class="caption">Conklin et al.&nbsp;(2018)</p></section>
<section id="eye-movements-blinks" class="slide level2">
<h2>Eye movements: Blinks</h2>
<p>By necessity, people blink.</p>
<ul>
<li>Variability across individuals.</li>
<li>Cognitive effort.</li>
<li>Usually, surrounded by saccades.</li>
<li>Pupil changes when eyelids open/close.</li>
</ul>
</section>
<section id="eye-movements-smooth-pursuit" class="slide level2">
<h2>Eye movements: Smooth pursuit</h2>
<p>A “moving fixation” ~ following a target.</p>
<ul>
<li>Slower than a saccade, but bounded by the velocity of the target being followed.</li>
<li>Asymmetrical: Horizontal &gt; vertical.</li>
</ul>
</section>
<section id="eye-movements-pupil-size" class="slide level2">
<h2>Eye movements: Pupil size</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Pupil: Absence of iris
<ul>
<li>3 mm, [2mm, 8 mm]</li>
<li>Changes because of mental processes 0.5 mm</li>
</ul></li>
<li>Size ~ amount of light that enters the eye
<ul>
<li>Smaller: sharper vision</li>
<li>Bigger: less acuity, but more light.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session1/eye_anatomy.png"></p>
</div>
</div>
<aside class="notes">
<p>here is some random information about the pupil. of particular interest is the differences in pupil size change induced by cognitive processes the bit about sharper vision has to do more with the aberrations induced by the lens and the liquid inside the eye Smaller pupil -&gt; depth of field increases, things appear in focus, decreases optical baerrations larger -&gt; more light into the eye, but less control over the rays, which decreases acuity</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-does-our-pupil-change-in-size-taxonomy" class="slide level2">
<h2>Why does our pupil change in size? Taxonomy</h2>
<div class="rows">
<div class="row" height="5%">
<ul>
<li>Taxonomy by Strauch et al.&nbsp;(2022)</li>
<li>Different networks</li>
</ul>
</div>
<div class="row" height="95%">
<div class="columns">
<div class="column" style="width:30%;">
<p><strong>Low-level</strong></p>
<ul>
<li>Pupillary light reflex</li>
<li>Pupil Near response</li>
<li>Dark reflex</li>
</ul>
</div><div class="column" style="width:40%;">
<p><strong>Intermediate-level</strong></p>
<ul>
<li>Alerting responses</li>
<li>Orienting responses</li>
</ul>
</div><div class="column" style="width:30%;">
<p><strong>High-level</strong></p>
<ul>
<li>Mental arousal</li>
</ul>
</div>
</div>
</div>
</div>
<aside class="notes">
<p>Although we want to draw conclusions about higher-level factors, we also need to keep these other factors in mind, especially when designing experiments e.g., play a beep before stimulus onset would lead to an orienting response</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-does-our-pupil-change-in-size-pupillary-light-reflex-plr" class="slide level2">
<h2>Why does our pupil change in size? Pupillary light reflex (PLR)</h2>
<p>Sequence: Constriction to a minimum -&gt; escape -&gt; return to normal</p>

<img data-src="_images/_session1/mathot2018_pupillightresponse.PNG" class="r-stretch quarto-figure-center"><p class="caption">Mathôt (2018)</p><aside class="notes">
<p>notice that this is a very big change! ~ 200 ms from light onset get them to think about profiles</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-does-our-pupil-change-in-size-taxonomy-1" class="slide level2">
<h2>Why does our pupil change in size? Taxonomy</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><strong>Low-level</strong></p>
<ul>
<li>Pupillary light reflex</li>
<li>Pupil Near response</li>
<li>Dark reflex</li>
</ul>
</div><div class="column" style="width:70%;">
<p>Reading/hearing words that convey brightness/darkness</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/mathotetal2017_embodiedcognition.PNG"></p>
<figcaption>Mathôt et al.&nbsp;(2017)</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="why-does-our-pupil-change-in-size-intermediate-level-processes" class="slide level2">
<h2>Why does our pupil change in size? Intermediate-level processes</h2>

<img data-src="_images/_session1/mathot2018_orientingresponse.PNG" class="r-stretch quarto-figure-center"><p class="caption">Mathôt (2018)</p><aside class="notes">
<p>a beep sounded. you can also see that there is a change in direction</p>
<p>Locus coeruleus (LC)-centered circuit for alerting</p>
<p>Superior colliculus (SC)-centered circuit for this</p>
<p>fast peak and then disappears novelty of a stimulus</p>
<p>for alerting a slightly different profile</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-does-our-pupil-change-in-size-higher-level-processes" class="slide level2">
<h2>Why does our pupil change in size? Higher-level processes</h2>

<img data-src="_images/_session1/mathot2018_workingmemory.PNG" class="r-stretch quarto-figure-center"><p class="caption">Mathôt (2018)</p></section>
<section id="eye-movements-pupil-size-1" class="slide level2 smaller">
<h2>Eye movements: Pupil size”</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Pupil size change as an index of “cognitive load”.</p>
<ul>
<li>Locus coeruleus and norepinephrine activity
<ul>
<li>Correlation of neuron firing activity in the LC and pupil diameter</li>
</ul></li>
</ul>
<p>In reality, a more complex picture.</p>
<ul>
<li>Cholinergic system</li>
<li>Serotonergic system</li>
<li>Dopaminergic system</li>
</ul>
<p>See Strauch et al.&nbsp;(2022); Joshi &amp; Gold (2020)</p>
</div><div class="column" style="width:50%;">
<p><br> <br></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/stracuhetal2022_proposednetworks.PNG"></p>
<figcaption>Strauch et al.&nbsp;(2022)</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="why-does-our-pupil-change-in-size-higher-level-processes-1" class="slide level2">
<h2>Why does our pupil change in size? Higher-level processes</h2>
<ul>
<li>Mental effort</li>
</ul>

<img data-src="_images/_session1/beatty_1982.PNG" class="r-stretch quarto-figure-center"><p class="caption">Kahneman &amp; Beatty (1966)</p></section>
<section id="why-does-our-pupil-change-in-size-higher-level-processes-2" class="slide level2">
<h2>Why does our pupil change in size? Higher-level processes</h2>
<ul>
<li>Allocation of resources</li>
</ul>
<p>Why is it important to think in terms of recruitment and allocation of resources.</p>

<img data-src="_images/_session1/chiew_braver.PNG" class="r-stretch quarto-figure-center"><p class="caption">Chiew &amp; Braver (2013)</p></section>
<section id="why-does-our-pupil-change-in-size-higher-level-processes-3" class="slide level2">
<h2>Why does our pupil change in size? Higher-level processes</h2>
<ul>
<li>Allocation of resources</li>
</ul>

<img data-src="_images/_session1/chiew_braver2.PNG" class="r-stretch quarto-figure-center"><p class="caption">Chiew &amp; Braver (2013)</p></section>
<section id="why-does-our-pupil-change-in-size-higher-level-processes-4" class="slide level2">
<h2>Why does our pupil change in size? Higher-level processes</h2>

<img data-src="_images/_session1/fink_review2.PNG" class="r-stretch quarto-figure-center"><p class="caption">Fink et al.&nbsp;(2023)</p></section>
<section id="eye-movements-pupil-size-2" class="slide level2">
<h2>Eye movements: Pupil size</h2>
<ul>
<li><p>Increasing interest in language research e.g., accented-speech comprehension.</p></li>
<li><p>Changes can take up to 3 s.</p>
<ul>
<li>Far longer than other eye events.</li>
<li>Requires very carefully designed experiments</li>
</ul></li>
</ul>
<p>Not the focus of this course.</p>
</section>
<section id="eye-movements-conceptualisation" class="slide level2">
<h2>Eye movements: Conceptualisation</h2>
<p>Field-specific</p>
<ul>
<li>Lots of measures &amp; linking hypotheses</li>
<li>Lots of ways to convey information</li>
</ul>
<aside class="notes">
<p>Obviously, when we conduct an ET exp for eye movements, we may use this measurements ‘raw’, but in some cases, they can be further divided, and how they link to a cognitive process may differ as a function of the field</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-movements-conceptualisation-1" class="slide level2">
<h2>Eye movements: Conceptualisation</h2>
<div class="rows">
<div class="row" height="40%">
<p>Lots of ways to convey information</p>
<ul>
<li><p>Example: Reading</p>
<ul>
<li>Fixations and saccades grouped into early/intermediate/late measures
<ul>
<li>Which map onto different hypothesized processes</li>
</ul></li>
</ul></li>
</ul>
</div>
<div class="row" height="60%">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/reiterate.JPG"></p>
<figcaption>Carroll (2017)</figcaption>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>Not going into this, I will only mention this briefly e.g., in reading, for example, the duration of fixations can be subdivided into the first fixation, the amount of fixations on an area of interest etc. and those are believed to map onto different stages of reading comprehension</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-movements-conceptualisation-2" class="slide level2">
<h2>Eye movements: Conceptualisation</h2>
<p>Eye movements can be later operationalised as a function of the research question.</p>
<ul>
<li>Space</li>
<li>Space x Time</li>
</ul>
</section>
<section id="eye-movements-space" class="slide level2">
<h2>Eye movements: Space</h2>
<p>We talk about <span class="fg" style="--col: #e64173">Areas of Interest (AOI)</span><sup>1</sup>.</p>
<ul>
<li>Use to explore eye movements around them.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/example_IA.jpg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<aside><ol class="aside-footnotes"><li id="fn3"><p>Or Regions of Interest (ROI), Interest Area (IA)</p></li></ol></aside></section>
<section id="eye-movements-space-x-time" class="slide level2">
<h2>Eye movements: Space x Time</h2>
<p>Speech comprehension</p>
<ul>
<li>When a fixation is triggered towards an object upon hearing its name.</li>
</ul>
<p>Reading</p>
<ul>
<li>Timed measures: first fixation duration, total reading time.
<ul>
<li>How long did a person spent reading a certain word the first time they saw it?</li>
</ul></li>
</ul>
</section>
<section id="eye-movements-conceptualisation-3" class="slide level2">
<h2>Eye movements: Conceptualisation</h2>
<div class="rows">
<div class="row" height="30%">
<p>Lots of ways to convey information</p>
<ul>
<li>Qualitative</li>
</ul>
</div>
<div class="row" height="70%">
<div class="columns">
<div class="column" style="width:33%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/bee_swarm.png"></p>
<figcaption>Bee swarm</figcaption>
</figure>
</div>
</div><div class="column" style="width:33%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/heatmap.png"></p>
<figcaption>Heatmap</figcaption>
</figure>
</div>
</div><div class="column" style="width:33%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/scanpath.png"></p>
<figcaption>Scanpath</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="eye-movements-conceptualisation-4" class="slide level2">
<h2>Eye movements: Conceptualisation</h2>
<div class="rows">
<div class="row" height="30%">
<ul>
<li>Quantitative</li>
</ul>
</div>
<div class="row" height="70%">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/fig3A_exp1elog.png" class="quarto-figure quarto-figure-center" height="500"></p>
<figcaption>Visual World Paradigm</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="eye-movements-conceptualisation-5" class="slide level2">
<h2>Eye movements: Conceptualisation</h2>
<div class="rows">
<div class="row" height="30%">
<ul>
<li>Quantitative</li>
</ul>
</div>
<div class="row" height="30%">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/reading3.png"></p>
<figcaption>Reading</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="eye-movements-tldr" class="slide level2">
<h2>Eye movements: TL;DR</h2>
<ul>
<li>There are five major eye movements (or events) that an eye-tracker can capture.</li>
<li>Fixations refer to ‘stable’ gazes on a space for a sustained period of time. We usually describe them in terms of how many they are, when they start, and how long they are.</li>
<li>Saccades are fast movements, commonly from one fixation to another. We describe them in terms of onset, offset, angle, velocity, latency, and acceleration.</li>
<li>Smooth pursuits are fixations that move.</li>
<li>Pupil size can change due to cognitive processing, whereby its size increases when effort is exerted.</li>
<li>In eye-tracking, we explore these eye movements in relation to time, space, and their interaction.</li>
</ul>
</section></section>
<section>
<section id="eye-tracking-in-language-research-1" class="title-slide slide level1 center">
<h1>Eye-tracking in language research</h1>

</section>
<section id="eye-tracking-in-psychology-research" class="slide level2">
<h2>Eye-tracking in psychology research</h2>
<p>Research traditions: Visual search (serial versus parallel processing), reading research (processed involved in text comprehension), scene perception (forming a representation of a scene), usability.</p>
<ul>
<li>Flash-preview moving-window paradigm, visual world paradigm, antisaccadic paradigm, saccadic mislocalization, social interaction paradigm, change blingness paradigm, gaze-contigent paradigms, preferential-looking paradigms, prosaccade paradigm…
<ul>
<li>&amp; combination of other paradigms with eye-tracking.</li>
</ul></li>
</ul>
</section>
<section id="example-scene-perception" class="slide level2">
<h2>Example: Scene perception</h2>
<ul>
<li>Scene perception: Presentation of images (complex/simple) to explore allocation of attention and visual processing.
<ul>
<li>Fixations on interesting and informative regions (Henderson, 2003)</li>
<li>Bottom-up versus top-down versus task-related</li>
</ul></li>
</ul>
<aside class="notes">
<p>Common research questions concern the extent to which various bottom-up or top-down factors explain where we direct our gaze in a scene, as well as how fast we can form a representation of the scene and recall it accurately. Since scenes are presented on a computer screen, researchers can directly manipulate and test low-level parameters such a luminance, colour, and contrast, as well as making detailed quantitative predictions from models. Typical measures are number of fixations and correlations between model-predicted and actual gaze locations. The scene may also be divided into areas of interest (AOIs), from which AOI measures and other eye movement statistics can be calculated</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example-scene-perception-1" class="slide level2">
<h2>Example: Scene perception</h2>

<img data-src="_images/_session1/hendersoncastellano.PNG" class="r-stretch quarto-figure-center"><p class="caption">Castelhano &amp; Henderson (2009)</p><aside class="notes">
<p>similar to yarbus huh</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eye-tracking-in-language-research-2" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>What are the eye-tracking paradigms in language research?</p>
<ul>
<li><p>Focus of this course:</p>
<ul>
<li>Visual World Paradigm</li>
<li>Single sentence reading</li>
</ul></li>
</ul>
</section>
<section id="eye-tracking-in-language-research-3" class="slide level2 smaller">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
<ul>
<li>VWP has predominantly been used to investigate speech comprehension as it unfolds.
<ul>
<li>Different levels of language comprehension (Huettig et al., 2011)</li>
</ul></li>
</ul>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Phonological level (e.g., Allopenna et al., 1998)</li>
<li>Lexical level (e.g., semantic prediction, Altmann &amp; Kamide, 1999)</li>
<li>Syntactic level (e.g., Knoeferle et al., 2005)</li>
<li>Discourse level (e.g., van Bergen &amp; Bosker, 2018)</li>
<li>Pragmatic level (e.g., Grodner et al., 2010)</li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li>Dialogue (e.g., Brown-Schmidt &amp; Tanenhaus, 2008)</li>
<li>Paralinguistic cues (e.g., Arnold et al., 2004)</li>
<li>Linguistic relativity (e.g., Papafragou et al., 2008).</li>
<li>Children</li>
<li>Aphasic patients (e.g., Mirman et al., 2011)</li>
<li>Non-native listeners (e.g., Ito et al., 2018)</li>
</ul>
</div>
</div>
</section>
<section id="eye-tracking-in-language-research-4" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
<p>Both verbal and non-verbal features exist when we produce and comprehend spoken language, and they can facilitate communication. Disfluencies (e.g., um or uh in English) are predominant in spontaneous speech.</p>
<ul>
<li>Do they affect listeners because of beliefs about disfluencies, or because of social reasoning?
<ul>
<li>Research question: Whether and how disfluent speech can be interpreted as deceitful, and whether there is an effect of interlocutors’ linguistic background (i.e., native versus non-native interlocutors).</li>
</ul></li>
</ul>
</section>
<section id="eye-tracking-in-language-research-5" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm: Treasure-hunt task</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="_images/_session1/exp.JPG"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="_images/_session1/example_loy2017.JPG"></p>
</div>
</div>
</section>
<section id="eye-tracking-in-language-research-6" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>

<img data-src="_images/_session1/expsdata.JPG" class="r-stretch"></section>
<section id="eye-tracking-in-language-research-7" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Visual World Paradigm</p>
<p>Conclusions:</p>
<ul>
<li>Participants made their decision shortly after the potential location was said (circa 400 ms).</li>
<li>The presence of a disfluency not only led to more interpretations of deceit, as reflected in the object selected, but also biased eye movements early.</li>
<li>There was no difference between native and non-native speakers.</li>
</ul>
<p>The interpretation of deceit triggered by disfluencies is inflexible, and likely heavily anchored in a stereotype of how deceit sounds (without any form of social reasoning).</p>
</section>
<section id="eye-tracking-in-language-research-8" class="slide level2 smaller">
<h2>Eye-tracking in language research</h2>
<p>Reading experiments</p>
<ul>
<li>Explore reading behaviour to make inferences about underlying linguistic processing mechanisms (lexical access, semantic and syntactic integration, prediction…)</li>
</ul>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Word recognition (e.g., frequency effect, Rayner &amp; Duffy, 1986)</li>
<li>Syntactic processing (e.g., structural ambiguities, Frazier &amp; Rayner, 1982)</li>
<li>Semantic integration (e.g., plausibility effect, Staub et al., 2007)</li>
<li>Predictive processing (e.g., Frisson et al., 2017)</li>
<li>Parafoveal processing (e.g., Juhasz et al., 2009)</li>
<li>Text comprehension (e.g., Dirix et al., 2019)</li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li>Multiword units (e.g., Carrol et al., 2016)</li>
<li>Children (e.g., Blythe et al., 2011)</li>
<li>Older adults (e.g., Solan et al., 1995)</li>
<li>Clinical populations
<ul>
<li>Children with dyslexia, Hyönä et al.&nbsp;(1995)</li>
<li>Adults with ASD, Howard et al.&nbsp;(2017)</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="eye-tracking-in-language-research-9" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<p>Constructions = grammatical patterns. Their productivity = the number of different words they can be used with. If productivity is high, there are many possible continuations that a sentence with this construction can have (example: “He started to…”). If productivity is low, there are very few possible continuations (example: “He burst into…”).</p>
<ul>
<li>In this low uncertainty situation, will a prediction error occur if readers see a continuation they did not expect?
<ul>
<li>Research question: Does low productivity of syntactic constructions lead to processing difficulty for unexpected lexical items?</li>
</ul></li>
</ul>
</section>
<section id="eye-tracking-in-language-research-10" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<p>High productivity condition:</p>
<ul>
<li>Francisco | <strong>se metió a</strong> | <span class="fg" style="--col: #e64173">hablar</span> | de política | con sus amigos</li>
</ul>
<p>Low productivity condition:</p>
<ul>
<li>Francisco | <strong>rompió a</strong> | <span class="fg" style="--col: #e64173">hablar</span> | de política | con sus amigos</li>
</ul>
</section>
<section id="eye-tracking-in-language-research-11" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>se metió a</strong> / <strong>rompió a</strong></p>
</div><div class="column" style="width:50%;">
<p><span class="fg" style="--col: #e64173">hablar</span></p>
</div>
</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/reading_resultsV.png" class="quarto-figure quarto-figure-center" height="350"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="_images/_session1/reading_resultsINF.png" class="quarto-figure quarto-figure-center" height="350"></p>
</figure>
</div>
</div>
</div>
</div>

<aside><div>
<p>Data from Mariia Baltais</p>
</div></aside></section>
<section id="eye-tracking-in-language-research-12" class="slide level2">
<h2>Eye-tracking in language research</h2>
<p>Reading</p>
<p>Conclusions:</p>
<ul>
<li>Low productivity was not associated with prediction error cost, so processing was not hindered by any unmet expectations.</li>
<li>Low productivity affected reading (integration and interpretation) of the construction/sentence as a whole.</li>
<li>Psycholinguistic evidence for the reality of the productivity phenomenon.</li>
</ul>
</section></section>
<section>
<section id="wrap-up" class="title-slide slide level1 center">
<h1>Wrap up</h1>

</section>
<section id="for-next-week" class="slide level2">
<h2>For next week</h2>
<ul>
<li>Groups (just send me an email)</li>
<li>T1 and T2 of Experiment Builder module</li>
<li>Remember that you can use the “forum” until next Tuesday at noon!
<ul>
<li>I will address the questions next Weds.</li>
</ul></li>
</ul>
</section>
<section id="references" class="slide level2 smaller">
<h2>References</h2>
<p>Alexander, R. G., &amp; Martinez-Conde, S. (2019). Fixational eye movements. <em>Eye movement research: An introduction to its scientific foundations and applications</em>, 73-115.</p>
<p>Allopenna, P. D., Magnuson, J. S., &amp; Tanenhaus, M. K. (1998). Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models. <em>Journal of memory and language,38</em>(4), 419-439.</p>
<p>Altmann, G. T., &amp; Kamide, Y. (1999). Incremental interpretation at verbs: Restricting the domain of subsequent reference. <em>Cognition, 73</em>(3), 247-264.</p>
<p>Arnold, J. E., Tanenhaus, M. K., Altmann, R. J., &amp; Fagnano, M. (2004). The old and thee, uh, new: Disfluency and reference resolution. <em>Psychological science, 15</em>(9), 578-582.</p>
<p>Blythe, H. I., Häikiö, T., Bertam, R., Liversedge, S. P., &amp; Hyönä, J. (2011). Reading disappearing text: Why do children refixate words?.<em>Vision research, 51</em>(1), 84-92.</p>
<p>Brown‐Schmidt, S., &amp; Tanenhaus, M. K. (2008). Real‐time investigation of referential domains in unscripted conversation: A targeted language game approach. <em>Cognitive science, 32</em>(4), 643-684.</p>
<p>Carrol, G., Conklin, K., &amp; Gyllstad, H. (2016). Found in translation: The influence of the L1 on the reading of idioms in a L2. <em>Studies in Second Language Acquisition, 38</em>(3), 403-443.</p>
</section>
<section id="references-1" class="slide level2 smaller">
<h2>References</h2>
<p>Carroll, T. (2017). Eye Behavior While Reading Words of Sanskrit and Urdu Origin in Hindi. Brigham Young University.</p>
<p>Castelhano, M. S., Mack, M. L., &amp; Henderson, J. M. (2009). Viewing task influences eye movement control during active scene perception. <em>Journal of Vision, 9</em>(3), 6-6.</p>
<p>Chiew, K. S., &amp; Braver, T. S. (2013). Temporal dynamics of motivation-cognitive control interactions revealed by high-resolution pupillometry. <em>Frontiers in psychology, 4</em>, 15.</p>
<p>Conklin, K., Pellicer-Sánchez, A., &amp; Carrol, G. (2018). <em>Eye-tracking: A guide for applied linguistics research</em>. Cambridge University Press.</p>
<p>Dirix, N., Vander Beken, H., De Bruyne, E., Brysbaert, M., &amp; Duyck, W. (2020). Reading text when studying in a second language: An eye‐tracking study. <em>Reading Research Quarterly,55</em>(3), 371-397.</p>
<p>Fink, L., Simola, J., Tavano, A., Lange, E., Wallot, S., &amp; Laeng, B. (2024). From pre-processing to advanced dynamic modeling of pupil data. <em>Behavior Research Methods, 56</em>(3), 1376-1412.</p>
<p>Frazier, L., &amp; Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. <em>Cognitive psychology, 14</em>(2), 178-210.</p>
</section>
<section id="references-2" class="slide level2 smaller">
<h2>References</h2>
<p>Frisson, S., Harvey, D. R., &amp; Staub, A. (2017). No prediction error cost in reading: Evidence from eye movements. <em>Journal of Memory and Language, 95</em>, 200-214.</p>
<p>Grodner, D. J., Klein, N. M., Carbary, K. M., &amp; Tanenhaus, M. K. (2010). “Some,” and possibly all, scalar inferences are not delayed: Evidence for immediate pragmatic enrichment. <em>Cognition, 116</em>(1), 42-55.</p>
<p>Henderson, J. M. (2003). Human gaze control during real-world scene perception. <em>Trends in cognitive sciences, 7</em>(11), 498-504.</p>
<p>Howard, P. L., Liversedge, S. P., &amp; Benson, V. (2017). Processing of co‐reference in autism spectrum disorder. <em>Autism Research, 10</em>(12), 1968-1980.</p>
<p>Hyönä, J., &amp; Olson, R. K. (1995). Eye fixation patterns among dyslexic and normal readers: effects of word length and word frequency. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition, 21</em>(6), 1430.</p>
<p>Ito, A., Corley, M., &amp; Pickering, M. J. (2018). A cognitive load delays predictive eye movements similarly during L1 and L2 comprehension. <em>Bilingualism: Language and Cognition, 21</em>(2), 251-264.</p>
<p>Joshi, S., &amp; Gold, J. I. (2020). Pupil size as a window on neural substrates of cognition. <em>Trends in cognitive sciences, 24</em>(6), 466-480.</p>
</section>
<section id="references-3" class="slide level2 smaller">
<h2>References</h2>
<p>Juhasz, B. J., Pollatsek, A., Hyönä, J., Drieghe, D., &amp; Rayner, K. (2009). Parafoveal processing within and between words. <em>Quarterly Journal of Experimental Psychology, 62</em>(7), 1356-1376.</p>
<p>Just, M. A., &amp; Carpenter, P. A. (1980). A theory of reading: from eye fixations to comprehension. <em>Psychological review, 87</em>(4), 329.</p>
<p>Kahneman, D., &amp; Beatty, J. (1966). Pupil diameter and load on memory. <em>Science, 154</em>(3756), 1583-1585.</p>
<p>Knoeferle, P., Crocker, M. W., Scheepers, C., &amp; Pickering, M. J. (2005). The influence of the immediate visual context on incremental thematic role-assignment: Evidence from eye-movements in depicted events. <em>Cognition, 95</em>(1), 95-127.</p>
<p>Mathôt, S. (2018). Pupillometry: Psychology, physiology, and function. <em>Journal of cognition, 1</em>(1).</p>
<p>Mathôt, S., Grainger, J., &amp; Strijkers, K. (2017). Pupillary responses to words that convey a sense of brightness or darkness. <em>Psychological science, 28</em>(8), 1116-1124.</p>
<p>Matin, E., Shao, K. C., &amp; Boff, K. R. (1993). Saccadic overhead: Information-processing time with and without saccades. <em>Perception &amp; psychophysics, 53</em>, 372-380.</p>
</section>
<section id="references-4" class="slide level2 smaller">
<h2>References</h2>
<p>Mirman, D., Yee, E., Blumstein, S. E., &amp; Magnuson, J. S. (2011). Theories of spoken word recognition deficits in aphasia: Evidence from eye-tracking and computational modeling. <em>Brain and language, 117</em>(2), 53-68.</p>
<p>Papafragou, A., Hulbert, J., &amp; Trueswell, J. (2008). Does language guide event perception? Evidence from eye movements. <em>Cognition, 108</em>(1), 155-184.</p>
<p>Pickering, M. J., Frisson, S., McElree, B., &amp; Traxler, M. J. (2004). Eye movements and semantic composition. <em>On-line study of sentence comprehension: Eyetracking, ERPs and beyond</em>, 33-50.</p>
<p>Rayner, K., &amp; Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. <em>Memory &amp; cognition, 14</em>(3), 191-201.</p>
<p>Solan, H. A., Feldman, J., &amp; Tujak, L. (1995). Developing visual and reading efficiency in older adults. <em>Optometry and Vision Science, 72</em>(2), 139-145.</p>
<p>Staub, A., Rayner, K., Pollatsek, A., Hyönä, J., &amp; Majewski, H. (2007). The time course of plausibility effects on eye movements in reading: evidence from noun-noun compounds. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition, 33</em>(6), 1162.</p>
<p>Strauch, C., Wang, C. A., Einhäuser, W., Van der Stigchel, S., &amp; Naber, M. (2022). Pupillometry as an integrated readout of distinct attentional networks. <em>Trends in Neurosciences, 45</em>(8), 635-647.</p>
</section>
<section id="references-5" class="slide level2 smaller">
<h2>References</h2>
<p>Van Bergen, G., &amp; Bosker, H. R. (2018). Linguistic expectation management in online discourse processing: An investigation of Dutch inderdaad’indeed’and eigenlijk’actually’. <em>Journal of Memory and Language, 103</em>, 191-209.</p>
<p>Yarbus, A. L. (1967). Eye movements and vision. New York: Plenum</p>
<div class="quarto-auto-generated-content">
<p><img src="_images/_session1/logo_ugent.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>Session 1 - Introduction to eye-tracking I</p>
</div>
</div>
</section></section>

    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="session_1_2025_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="session_1_2025_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="session_1_2025_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="session_1_2025_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="session_1_2025_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="session_1_2025_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="session_1_2025_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="session_1_2025_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="session_1_2025_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="session_1_2025_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    

</body></html>